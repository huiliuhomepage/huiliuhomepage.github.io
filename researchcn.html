<html lang="cn">
	<head>
		<meta charset="utf-8">
		<title>刘辉的学术经历（0000-0002-6850-9570）</title>
		<style>
			html, body, form, fieldset, p, div, h1, h2, h3, h4, h5, h6 {-webkit-text-size-adjust:100%;}
			body {font: normal 16px Verdana, Arial, sans-serif;}
			img {transition: transform .5s;}
			img:hover {transform: scale(2);}
			.slightpop:hover {transform: scale(1.05);}
			.pop1:hover {transform: scale(1.6);}
			.pop5:hover {transform: scale(5);}
			.pop10:hover {transform: scale(10);}
			.pop15:hover {transform: scale(15);}
			.pop20:hover {transform: scale(20);}
			.pop25:hover {transform: scale(25);}
			.pop40:hover {transform: scale(40);}
			.round {border-radius: 50%; overflow: hidden;}
		</style>
	</head>

	<body background="images/Arrowhead.svg" link="#000000" alink="#0080FF" vlink = "#000000">
		<div class="main" id="home">
			<br>
			<h3 align="left"><font face="sans" size="5">
				&nbsp;&nbsp;<a href="indexcn.html">首 页</a>&nbsp;&nbsp;&nbsp;&nbsp;
				<a>学 术</a>&nbsp;&nbsp;&nbsp;&nbsp;
				<a href="artcn.html">艺 术</a>&nbsp;&nbsp;&nbsp;&nbsp;
				<a href="academictreecn.html">学术族谱</a>&nbsp;&nbsp;&nbsp;&nbsp;
				<a href="arttreecn.html">艺术族谱</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
				<a href="research.html"><img src="images/EN.svg" height="15"></a>
			</font></h3>
		</div>

		<br><br><br><br><br>

		<table align="left" cellpadding="15">
			<tr><td><h1 align="left"><span style="text-decoration: overline">学术链接</span></h1></td></tr>
			<tr><td><table align="left" cellpadding="15">
				<tr>
					<td>
						<a href="https://www.uni-bremen.de/en/csl/institute/team/staff/dr-hui-liu" target="_blank" rel="noopener noreferrer"><img src="images/EN.svg" height="20" class="pop2"></a>&nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://www.uni-bremen.de/csl/institut/team/mitarbeitende/dr-hui-liu" target="_blank" rel="noopener noreferrer"><img src="images/DE.svg" height="20" class="pop2"></a>
					</td>
					<td><b>个人学术主页@不来梅大学</b></td>
				</tr>
				<tr><td><a href="https://orcid.org/0000-0002-6850-9570" target="_blank" rel="noopener noreferrer"><img src="images/ORCID.png" height="20" class="pop2"></a></td><td>0000-0002-6850-9570</td></tr>
				<tr><td><a href="https://scholar.google.com/citations?user=GyLOsRwAAAAJ" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="20" class="pop2"></a></td><td>GyLOsRwAAAAJ</td></tr>
				<tr><td><a href="https://www.researchgate.net/profile/Hui-Liu-149" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="20" class="pop2"></a></td><td>Hui-Liu-149</td></tr>
				<tr><td><a href="https://www.scopus.com/authid/detail.uri?authorId=57196004640" target="_blank" rel="noopener noreferrer"><img src="images/Scopus.png" height="20" class="pop2"></a><td>57196004640</td></td></tr>
				<tr><td><a href="https://www.semanticscholar.org/author/Hui-Liu/2146672447" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="20" class="pop2"></a></td><td>2146672447</td></tr>
				<tr><td><a href="https://www.webofscience.com/wos/author/record/ACZ-9903-2022" target="_blank" rel="noopener noreferrer"><img src="images/WoS.png" height="20" class="pop2"></a></td><td>ACZ-9903-2022</td></tr>
				<tr><td></td></tr>
				<tr><td>欢迎您将最新学术成果投稿：</td></tr>
				<tr>
					<td><a href="https://www.mdpi.com/journal/biosensors/special_issues/70678AF74D" target="_blank" rel="noopener noreferrer"><img src="images/Biosensors.png" height="40"></a></td>
					<td>《采集、处理、建模或利用生物医学和生理信号的新颖技术》</td>
				</tr>
				<tr>
					<td><a href="https://www.mdpi.com/journal/sensors/special_issues/L9E9BHCLP8" target="_blank" rel="noopener noreferrer"><img src="images/Sensors.png" height="40"></a></td>
					<td>《肌电和脑电传感技术在生物医学领域的应用》</td>
				</tr>
				<tr>
					<td>
						<a href="https://www.mdpi.com/journal/biosensors/special_issues/F99X76258C" target="_blank" rel="noopener noreferrer"><img src="images/Biosensors.png" height="40"></a>
						<a href="https://www.mdpi.com/journal/sensors/special_issues/Q78J80G0OP" target="_blank" rel="noopener noreferrer"><img src="images/Sensors.png" height="40"></a>
					</td>
					<td>《面向人体动作的传感器研究 卷III》</td>
				</tr>
			</table></td></tr>
			
			<tr><td></td></tr><tr><td></td></tr>

			<tr><td><h1 align="left"><span style="text-decoration: overline">研究领域</span></h1></td></tr>
			<tr><td><table align="left" cellpadding="15">
				<tr>
					<td>
						<a href="images/KeywordsCN.png" target="_blank" rel="noopener noreferrer"><img src="images/KeywordsCN.png" height = "400" class="slightpop"></a><br>
						谷歌“世界科学家影响力按领域标签排名”：<br>
						&#9679; 生物信号 <a href="https://scholar.google.com/citations?view_op=search_authors&mauthors=label:biosignals" target="_blank" rel="noopener noreferrer">Biosignals</a> (13)<br>
						&#9679; 隐马尔可夫模型 <a href="https://scholar.google.com/citations?view_op=search_authors&mauthors=label:hidden_markov_model" target="_blank" rel="noopener noreferrer">Hidden Markov Models</a> (3)<br>
						&#9679; 多源传感器数据融合 <a href="https://scholar.google.com/citations?view_op=search_authors&mauthors=label:sensor_data_fusion" target="_blank" rel="noopener noreferrer">Sensor Data Fusion</a> (11)<br>
						&#9679; 中国音乐 <a href="https://scholar.google.com/citations?view_op=search_authors&mauthors=label:chinese_music" target="_blank" rel="noopener noreferrer">Chinese Music</a> (1)<br>
						&#9679; 智慧建筑 <a href="https://scholar.google.com/citations?view_op=search_authors&mauthors=label:smart_building" target="_blank" rel="noopener noreferrer">Smart Building</a> (15)
					</td>
					<td>
					<!--
						<video width="560" controls="controls">
							<source src="https://seafile.zfn.uni-bremen.de/f/6c96c91c878d4d1caa66/?dl=1" type="video/mp4" />
						</video>
						 （<a href="https://www.youtube.com/embed/MseG4XWuGzs" target="_blank" rel="noopener noreferrer"><img src="images/YouTube.png" height="13"></a>）<br><br>
					-->
						<a href="images/Bandage_Demo.jpg" target="_blank" rel="noopener noreferrer"><img src="images/Bandage_Demo.jpg" height = "400" class="slightpop"></a><br>
						<br>研发出第一款辅助医疗康复的智能膝盖绷带，实现了：<br>
						&#9679; 多模态可穿戴式传感器在膝盖绷带上的集成方案<br>
						&#9679; 多源异构数据实时处理软件平台<br>
						&#9679; 实时人体动作识别<br>
						12届国际生医工程系统和技术联合会议 <a href="https://biostec.scitevents.org/PreviousAwards.aspx#2019" target="_blank" rel="noopener noreferrer"><img src="images/BIOSTEC2019.png" height="12"></a> <a href="https://biostec.scitevents.org/PreviousAwards.aspx#2019" target="_blank" rel="noopener noreferrer">最佳论文</a>（学生作者）	
						<img src="images/BestPaperAward.png" height="12" class="pop40"> <img src="images/BestPaper.png" height="12" class="pop25">
					</td>
				</tr>
			</table></td></tr>

			<tr><td></td></tr><tr><td></td></tr>
		
			<tr><td><h1 align="left"><span style="text-decoration: overline">论文发表（88）</span></h1> SCI总影响因子=235.2，其中一作/通讯总影响因子=200.4</td></tr>
			<tr><td><table align="left" cellpadding="15">
			
				<tr><td>
					<img src="images/Book.png" height="15">&nbsp;&nbsp;|&nbsp;&nbsp;<img src="images/Dissertation.png" height="15">&nbsp;&nbsp;
					<img src="images/5.png" height="15">
				</td></tr>
				<tr><td>
					<i>Biomedical Engineering Systems and Technologies.</i>  Guarino, M. P., Hotta, K., Yousef, M., Liu, H., Saggio, G., Fred, A., and Gamboa, H. (eds., 2025). Springer Nature Switzerland.<br>
					<a href="https://www.springerprofessional.de/biomedical-engineering-systems-and-technologies/51405216" target="_blank" rel="noopener noreferrer"><img src="images/Springer_Professional.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/book/10.1007/978-3-031-96899-0" target="_blank" rel="noopener noreferrer"><img src="images/SpringerLink.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1007/978-3-031-96899-0" target="_blank" rel="noopener noreferrer">10.1007/978-3-031-96899-0</a>
					<!--
					<a href="https://scholar.google.com/scholar?q=" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					-->
				</td></tr>
				<tr><td>
					<i>Sensors for Human Activity Recognition II.</i> Liu, H., Gamboa, H., and Schultz, T. (eds., 2025). MDPI.<br>
					<a href="https://scholar.google.com/scholar?q=Sensors+for+Human+Activity+Recognition+II+Hui+Liu" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.amazon.com/Sensors-Human-Activity-Recognition-II/dp/3725828032" target="_blank" rel="noopener noreferrer"><img src="images/Amazon.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/388523690_Sensors_for_Human_Activity_Recognition_II" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/books/book/10390" target="_blank" rel="noopener noreferrer"><img src="images/MDPIBooks.svg" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/books978-3-7258-2804-3" target="_blank" rel="noopener noreferrer">10.3390/books978-3-7258-2804-3</a>
				</td></tr>
				<tr><td>
					<i>Proceedings of the 17th International Joint Conference on Biomedical Engineering Systems and Technologies – (Volume 1).</i> Guarino, M. P., Hotta, K., Yousef, M., Liu, H., Saggio, G., Fred, A., and Gamboa, H. (eds., 2024). INSTICC.<br>
					<a href="https://www.semanticscholar.org/paper/Proceedings-of-the-17th-International-Joint-on-and-Liu-Liu/3e51838ee78dad4fec3b404223b02913a9f3adc5" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/378935961_Proceedings_of_the_17th_International_Joint_Conference_on_Biomedical_Engineering_Systems_and_Technologies_-_Volume_1" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://doi.org/10.5220/0000184700003657" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0000184700003657" target="_blank" rel="noopener noreferrer">10.5220/0000184700003657</a>
				</td></tr>
				<tr><td>
					<i>Sensors for Human Activity Recognition.</i> Liu, H., Gamboa, H., and Schultz, T. (eds., 2023). MDPI.<br>
					<a href="https://scholar.google.com/scholar?q=Sensors+for+Human+Activity+Recognition+Hui+Liu" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.google.de/books/edition/Sensors_for_Human_Activity_Recognition/jTID0AEACAAJ" target="_blank" rel="noopener noreferrer"><img src="images/GoogleBooks.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.amazon.com/Sensors-Human-Activity-Recognition/dp/3036575545" target="_blank" rel="noopener noreferrer"><img src="images/Amazon.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/372077993_Sensors_for_Human_Activity_Recognition" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/books/book/7447" target="_blank" rel="noopener noreferrer"><img src="images/MDPIBooks.svg" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/books978-3-0365-7555-1" target="_blank" rel="noopener noreferrer">10.3390/books978-3-0365-7555-1</a>
				</td></tr>
				<tr><td>
					<i>Biosignal Processing and Activity Modeling for Multimodal Human Activity Recognition.</i> Liu, H. (2021). PhD thesis, University of Bremen.<br>
					<a href="https://scholar.google.com/scholar?q=Biosignal+Processing+and+Activity+Modeling+for+Multimodal+Human+Activity+Recognition" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Biosignal-processing-and-activity-modeling-for-Liu/4bc9489c1b9169feeef22a1bda902e3bb18b0ebb" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/356683096_Biosignal_Processing_and_Activity_Modeling_for_Multimodal_Human_Activity_Recognition" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://theses.eurasip.org/theses/911/biosignal-processing-and-activity-modeling-for/" target="_blank" rel="noopener noreferrer"><img src="images/Eurasip.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://media.suub.uni-bremen.de/handle/elib/5492" target="_blank" rel="noopener noreferrer"><img src="images/SuUB.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.26092/elib/1219" target="_blank" rel="noopener noreferrer">10.26092/elib/1219</a>
				</td></tr>
			
				<tr><td>
				</td></tr>
			
				<tr><td>
					<img src="images/Chapter.png" height="15">&nbsp;&nbsp;
					<img src="images/4.png" height="15">
				</td></tr>
				<tr><td>
					<img src="images/Forthcoming.png" height="20" class = "pop1">
					<i>EMG-based Action Unit Recognition of Subtle and Intense Expressions: Towards Single-Trial Classification Models.</i> Küster, D.*, Rammonhan, R. A., Liu, H., Schultz, T., Koschke, R. (2026). Biomedical Engineering Systems and Technologies, Springer Nature Switzerland.<br>
					<!--
					<a href="https://scholar.google.com/scholar?q=" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/" target="_blank" rel="noopener noreferrer"></a>
					-->
				</td></tr>
				<tr><td>
					<i>Muscle Synergy and Co-Contraction Effects on Joystick Manipulation.</i> Ouyang, C., Cai, L.*, Yan, S., Zhang, T., Zhu, J., Chen, L., and Liu, H.* (2025). Biomedical Engineering Systems and Technologies, Springer Nature Switzerland.<br>
					<a href="https://scholar.google.com/scholar?q=Muscle+Synergy+and+Co-Contraction+Effects+on+Joystick+Manipulation" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					<!--
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/High-Level-Features-for-Human-Activity-Recognition-Hartmann-Liu/d247cc3dbe4ea9673b8f2bebdd62a10eef42d978" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					-->
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/395117308_Muscle_Synergy_and_Co-contraction_Effects_on_Joystick_Manipulation" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.springerprofessional.de/muscle-synergy-and-co-contraction-effects-on-joystick-manipulati/51405238" target="_blank" rel="noopener noreferrer"><img src="images/Springer_Professional.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/chapter/10.1007/978-3-031-96899-0_16" target="_blank" rel="noopener noreferrer"><img src="images/SpringerLink.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1007/978-3-031-96899-0_16" target="_blank" rel="noopener noreferrer">10.1007/978-3-031-96899-0_16</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/book/10.1007/978-3-031-96899-0" target="_blank" rel="noopener noreferrer"><img src="images/Book.png" height="15"></a>
				</td></tr>
				<tr><td>
					<i>EMG-Based Action Unit Recognition: Feature Engineering, Machine Learning, and Real-Time Classification.</i> Liu, H.*, Veldanda, A., Koschke, R., Schultz, T., and Küster, D.* (2025). Biomedical Engineering Systems and Technologies, Springer Nature Switzerland.<br>
					<a href="https://scholar.google.com/scholar?q=EMG-Based+Action+Unit+Recognition%3A+Feature+Engineering%2C+Machine+Learning%2C+and+Real-Time+Classification" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					<!--
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/High-Level-Features-for-Human-Activity-Recognition-Hartmann-Liu/d247cc3dbe4ea9673b8f2bebdd62a10eef42d978" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					-->
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/395118095_EMG-Based_Action_Unit_Recognition_Feature_Engineering_Machine_Learning_and_Real-Time_Classification" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.springerprofessional.de/emg-based-action-unit-recognition-feature-engineering-machine-le/51405266" target="_blank" rel="noopener noreferrer"><img src="images/Springer_Professional.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/chapter/10.1007/978-3-031-96899-0_3" target="_blank" rel="noopener noreferrer"><img src="images/SpringerLink.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1007/978-3-031-96899-0_3" target="_blank" rel="noopener noreferrer">10.1007/978-3-031-96899-0_3</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/book/10.1007/978-3-031-96899-0" target="_blank" rel="noopener noreferrer"><img src="images/Book.png" height="15"></a>
				</td></tr>
				<tr><td>
					<i>High-Level Features for Human Activity Recognition and Modeling.</i> Hartmann, Y.*, Liu, H., and Schultz, T. (2023). Biomedical Engineering Systems and Technologies. Springer Nature Switzerland.<br>
					<a href="https://scholar.google.com/scholar?q=High-Level+Features+for+Human+Activity+Recognition+and+Modeling" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/High-Level-Features-for-Human-Activity-Recognition-Hartmann-Liu/d247cc3dbe4ea9673b8f2bebdd62a10eef42d978" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/372525464_High-Level_Features_for_Human_Activity_Recognition_and_Modeling" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.springerprofessional.de/high-level-features-for-human-activity-recognition-and-modeling/25835520" target="_blank" rel="noopener noreferrer"><img src="images/Springer_Professional.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/chapter/10.1007/978-3-031-38854-5_8" target="_blank" rel="noopener noreferrer"><img src="images/SpringerLink.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1007/978-3-031-38854-5_8" target="_blank" rel="noopener noreferrer">10.1007/978-3-031-38854-5_8</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/book/10.1007/978-3-642-18472-7" target="_blank" rel="noopener noreferrer"><img src="images/Book.png" height="15"></a>
				</td></tr>
			
				<tr><td>
				</td></tr>
			
				<tr><td>
					<img src="images/Journal.png" height="15">&nbsp;&nbsp;
					<img src="images/5.png" height="15"><img src="images/6.png" height="15">&nbsp;&nbsp;|&nbsp;
					<img src="images/Hot_Paper.png" height="15" class = "pop1"> <img src="images/8.png" height="15"> (top 0.1% on <img src="images/WoS.png" height="15" class = "pop1">)&nbsp;&nbsp;|&nbsp;
					<img src="images/Highly_Cited_Paper.png" height="15" class = "pop1"> <img src="images/8.png" height="15"> (top 1%)
				</td></tr>
				<tr><td>
					<i>Taxonomy and Real-Time Classification of Artifacts during Biosignal Acquisition: A Starter Study and Dataset of ECG.</i> Liu, H.*, Zhang, S., Gamboa, H., Xue, T., Zhou, C., and Schultz, T. (2024). IEEE Sensors Journal, 24(6):9162-9171. ISSN: 1530-437X.<br>
					<img src="images/Hot.png" height="15" class = "pop1"> <img src="images/Highly.png" height="15" class = "pop1">
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/scholar?q=Taxonomy+and+Real-Time+Classification+of+Artifacts+during+Biosignal+Acquisition%3A+A+Starter+Study+and+Dataset+of+ECG" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Taxonomy-and-Real-Time-Classification-of-Artifacts-Liu-Zhang/e499518389b2b51b5f4b2a41610f7cf7037818be" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/377731834_Taxonomy_and_Real-Time_Classification_of_Artifacts_during_Biosignal_Acquisition_A_Starter_Study_and_Dataset_of_ECG" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/10415350" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.uni-bremen.de/en/csl/research/sensorder-artifact-classification-during-biosignal-acquisition" target="_blank" rel="noopener noreferrer"><img src="images/Dataset.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/JSEN.2024.3356651" target="_blank" rel="noopener noreferrer">10.1109/JSEN.2024.3356651</a>
				</td></tr>
				<tr><td>
					<i>MS2OD: Outlier Detection Using Minimum Spanning Tree and Medoid Selection.</i> Li, J., Li, JW, Wang, C., Verbeek F. J.*, Schultz, T., and Liu, H.* (2024). Machine Learning: Science and Technology, 5:015025. IOPscience. ISSN: 2632-2153.<br>
					<img src="images/Hot.png" height="15" class = "pop1"> <img src="images/Highly.png" height="15" class = "pop1">
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/scholar?q=MS2OD%3A+Outlier+Detection+Using+Minimum+Spanning+Tree+and+Medoid+Selection" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/MS2OD%3A-Outlier-Detection-Using-Minimum-Spanning-and-Li-Li/9f2f5a172ec1f1cbd22e522c336282010beeb283" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/377861624_MS2OD_Outlier_Detection_Using_Minimum_Spanning_Tree_and_Medoid_Selection" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://iopscience.iop.org/article/10.1088/2632-2153/ad2492" target="_blank" rel="noopener noreferrer"><img src="images/IOP.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1088/2632-2153/ad2492" target="_blank" rel="noopener noreferrer">10.1088/2632-2153/ad2492</a>
				</td></tr>
				<tr><td>
					<i>Muscle Synergies in Joystick Manipulation.</i> Cai, L., Yan, S., Ouyang C., Zhang, T., Zhu, J., Chen, L., Ma, X.*, and Liu, H.* (2023). Frontiers in Physiology, 14:1282295. ISSN: 1664-042X.<br>
					<img src="images/Hot.png" height="15" class = "pop1"> <img src="images/Highly.png" height="15" class = "pop1">
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/scholar?q=Muscle+Synergies+in+Joystick+Manipulation" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Muscle-synergies-in-joystick-manipulation-Cai-Yan/b8f4cacd594fed0ec79a4f7b966d22673e864719" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/374675206_Muscle_synergies_in_joystick_manipulation" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/articles/10.3389/fphys.2023.1282295" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fphys.2023.1282295" target="_blank" rel="noopener noreferrer">10.3389/fphys.2023.1282295</a>
				</td></tr>
				<tr><td>
					<i>Outlier Detection Using Iterative Adaptive Mini-Minimum Spanning Tree Generation with Applications on Medical Data.</i> Li, J., Li, JW, Wang, C, Verbeek, F. J.*, Schultz T., and Liu H.* (2023). Frontiers in Physiology, 14:1233341. ISSN: 1664-042X.<br>
					<img src="images/Hot.png" height="15" class = "pop1"> <img src="images/Highly.png" height="15" class = "pop1">
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/scholar?q=Outlier+Detection+Using+Iterative+Adaptive+Mini-Minimum+Spanning+Tree+Generation+with+Applications+on+Medical+Data" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Outlier-detection-using-iterative-adaptive-spanning-Li-Li/fb0fc149b382ef5ce415b44b35ab7352a0eb4b46" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/374675183_Outlier_detection_using_iterative_adaptive_mini-minimum_spanning_tree_generation_with_applications_on_medical_data" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/articles/10.3389/fphys.2023.1233341" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fphys.2023.1233341" target="_blank" rel="noopener noreferrer">10.3389/fphys.2023.1233341</a>
				</td></tr>
				<tr><td>
					<i>Robust Human Locomotion and Localization Activity Recognition over Multisensory.</i> Khan, D., Alonazi, M., Abdelhaq, M.*, Al Mudawi, N., Algarni, A., Jalal, A.*, and Liu, H.* (2024). Frontiers in Physiology, 15:1344887. ISSN: 1664-042X.<br>
					<img src="images/Hot.png" height="15" class = "pop1"> <img src="images/Highly.png" height="15" class = "pop1">
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.de/scholar?q=Robust+Human+Locomotion+and+Localization+Activity+Recognition+over+Multisensory" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Robust-human-locomotion-and-localization-activity-Khan-Alonazi/bb90b2c6698b935dcc789faab546118be8717927" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/378393968_Robust_human_locomotion_and_localization_activity_recognition_over_multisensory" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/physiology/articles/10.3389/fphys.2024.1344887" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fphys.2024.1344887" target="_blank" rel="noopener noreferrer">10.3389/fphys.2024.1344887</a>
				</td></tr>
				<tr><td>
					<i>Efficient Wi-Fi-Based Human Activity Recognition Using Adaptive Antenna Elimination.</i> Jannat, M.K.A., Islam, M.S., Yang, S-H*, and Liu, H.* (2023). IEEE Access, 11:105440-105454.<br>
					<img src="images/Hot.png" height="15" class = "pop1"> <img src="images/Highly.png" height="15" class = "pop1">
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/scholar?q=Efficient+Wi-Fi-Based+Human+Activity+Recognition+Using+Adaptive+Antenna+Elimination" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Efficient-Wi-Fi-Based-Human-Activity-Recognition-Jannat-Islam/02ff217d1dd6316596fc1da4b8f6a107ee5b7f06" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/374244149_Efficient_Wi-Fi-Based_Human_Activity_Recognition_Using_Adaptive_Antenna_Elimination" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/10265041" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/ACCESS.2023.3320069" target="_blank" rel="noopener noreferrer">10.1109/ACCESS.2023.3320069</a>
				</td></tr>
				<tr><td>
					<i>Biosensor-Driven IoT Wearables for Accurate Body Motion Tracking and Localization.</i> Almujally, N. A., Khan, D., Al Mudawi, N, Alonazi, M., Alazeb, A., Algarni, A., Jalal, A.*, and Liu, H.* (2024). Sensors 24(10):3032.<br>
					<img src="images/Hot.png" height="15" class = "pop1"> <img src="images/Highly.png" height="15" class = "pop1">
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/scholar?q=Biosensor-Driven+IoT+Wearables+for+Accurate+Body+Motion+Tracking+and+Localization" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Biosensor-Driven-IoT-Wearables-for-Accurate-Body-Almujally-Khan/ed94ce0e09d14206f87f1c86433e793c0a383659" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/380552972_Biosensor-Driven_IoT_Wearables_for_Accurate_Body_Motion_Tracking_and_Localization" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/1424-8220/24/10/3032" target="_blank" rel="noopener noreferrer"><img src="images/Sensors.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/s24103032" target="_blank" rel="noopener noreferrer">10.3390/s24103032</a>
				</td></tr>
				<tr><td>
					<i>Remote Intelligent Perception System for Multi-Object Detection.</i> Alazeb, A., Chughtai, B. R., Al Mudawi, N., Alqahtani, Y., Alonazi, M., Aljuaid, H., Jalal, A.*, and Liu, H.* (2024). Frontiers in Neurorobotics, 18:1398703. ISSN: 1662-5218.<br>
					<img src="images/Hot.png" height="15" class = "pop1"> <img src="images/Highly.png" height="15" class = "pop1">
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/scholar?q=Remote+intelligent+perception+system+for+multi-object+detection" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Remote-intelligent-perception-system-for-detection-Alazeb-Chughtai/ac27d489678a64b99940afba827eaab3278711fd" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/380720805_Remote_intelligent_perception_system_for_multi-object_detection" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/articles/10.3389/fnbot.2024.1398703" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fnbot.2024.1398703" target="_blank" rel="noopener noreferrer">10.3389/fnbot.2024.1398703</a>
				</td></tr>
			
				<tr><td>
					<img src="images/Forthcoming.png" height="20" class = "pop1">
					<i>UAV-Based Intelligent Traffic Surveillance using Recurrent Neural Networks and Swin Transformer for Dynamic Environments.</i> Almujally, N. A., Wu, T., Alhasson, H. F., Hanzla, M., Jalal, A.*, and Liu, H.* （2025）.  Frontiers in Neurorobotics. ISSN: 1662-5218.<br>
					<a href="https://scholar.google.com/scholar?q=UAV-Based+Intelligent+Traffic+Surveillance+using+Recurrent+Neural+Networks+and+Swin+Transformer+for+Dynamic+Environments" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					<!--
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					-->
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2025.1681341" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fnbot.2025.1681341" target="_blank" rel="noopener noreferrer">10.3389/fnbot.2025.1681341</a>
				</td></tr>
				<tr><td>
					<img src="images/Forthcoming.png" height="20" class = "pop1">
					<i>Aerial Images for Intelligent Vehicle Detection and Classification via YOLOv11 and Deep Learner.</i> Mujtaba, G., Liu, W., Alshehri, M., AlQahtani, Y., Almujally, N. A., and Liu, H.* (2025). Computer, Materials & Continua. Tech Science Press. ISSN: 1546-2218.<br>
					<!--
					<a href="https://scholar.google.com/scholar?q=" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/" target="_blank" rel="noopener noreferrer"></a>
					-->
				</td></tr>
			
				<tr><td>
					<i>Integrated Neural Network Framework for Multi-Object Detection and Recognition Using UAV Imagery.</i> Alshehri, M., Xue, T., Mujtaba, G., Alqahtani, Y., Almujally, N., Jalal, A.*, Liu, H.* (2025). Frontiers in Neurorobotics. ISSN: 1662-5218.<br>
					<a href="https://scholar.google.com/scholar?q=Integrated+Neural+Network+Framework+for+Multi-Object+Detection+and+Recognition+Using+UAV+Imagery" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Integrated-neural-network-framework-for-detection-Alshehri-Xue/2b0e0abfce197f662ff657f571967303558a950f" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/394102368_Integrated_neural_network_framework_for_multi-object_detection_and_recognition_using_UAV_imagery" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2025.1643011" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fnbot.2025.1643011" target="_blank" rel="noopener noreferrer">10.3389/fnbot.2025.1643011</a>
				</td></tr>
				<tr><td>
					<i>ANN-Assisted Optimization of a Solar-to-X System for Green Hydrogen Production, CO₂ Capture, and Methanol-Based Energy Storage.</i> Liu, H.*, Liu, W., Wu, Y., Du, Z., Xue, T., Manesh A. M., Xu, J*, and Jiang, T.* (2025). Energy, 333:137350. Elsevier. ISSN: 0360-5442.<br>
					<a href="https://scholar.google.com/scholar?q=ANN-Assisted+Optimization+of+a+Solar-to-X+System+for+Green+Hydrogen+Production%2C+CO2+Capture%2C+and+Methanol-Based+Energy+Storage" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/ANN-Assisted-Optimization-of-a-Solar-to-X-System-Liu-Liu/3bd6393294dcff48fc1198229233feb31e66698e" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/393284049_ANN-Assisted_Optimization_of_a_Solar-to-X_System_for_Green_Hydrogen_Production_CO2_Capture_and_Methanol-Based_Energy_Storage" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0360544225029925" target="_blank" rel="noopener noreferrer"><img src="images/Elsevier.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1016/j.energy.2025.137350" target="_blank" rel="noopener noreferrer">10.1016/j.energy.2025.137350</a>
				</td></tr>
				<tr><td>
					<i>A New Synthesized Framework of Artificial Neural Network-Based Sensitivity Analysis for Building Energy Performance: A Case Study of Shanghai, China.</i> Liu, H.*, Wu, Y., Liu, W., Xue, T., Xu, J. (2025). Building and Environment, 283:113290. Elsevier. ISSN: 0360-1323.<br>
					<a href="https://scholar.google.com/scholar?q=A+new+synthesized+framework+of+artificial+neural+network-based+sensitivity+analysis+for+building+energy+performance%3A+A+case+study+of+Shanghai" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/A-new-synthesized-framework-of-artificial-neural-A-Liu-Wu/35a093fac14243a3218d9ba045e9f02e11db47ef" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/392848290_A_new_synthesized_framework_of_artificial_neural_network-based_sensitivity_analysis_for_building_energy_performance_A_case_study_of_Shanghai_China" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S036013232500770X" target="_blank" rel="noopener noreferrer"><img src="images/Elsevier.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1016/j.buildenv.2025.113290" target="_blank" rel="noopener noreferrer">10.1016/j.buildenv.2025.113290</a>
				</td></tr>
				<tr><td>
					<i>A Hybrid Deep Learning Pipeline for Wearable Sensors-based Human Activity Recognition.</i> Algarni, A., Abro, I. A., Alshehri, M., AlQahtani, Y., Alshahrani, A., and Liu, H.* (2025). Computer, Materials & Continua. Tech Science Press. ISSN: 1546-2218.<br>
					<a href="https://scholar.google.com/scholar?q=A+Hybrid+Deep+Learning+Pipeline+for+Wearable+Sensors-based+Human+Activity+Recognition" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/A-Hybrid-Deep-Learning-Pipeline-for-Wearable-Human-Algarni-Aijaz/0ea22b20f494c1cb281278eefe4f0f23f27ebd3a" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/393667197_A_Hybrid_Deep_Learning_Pipeline_for_Wearable_Sensors-Based_Human_Activity_Recognition" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.techscience.com/cmc/online/detail/23774" target="_blank" rel="noopener noreferrer"><img src="images/Tech_Science_Press.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.32604/cmc.2025.064601" target="_blank" rel="noopener noreferrer">10.32604/cmc.2025.064601</a>
				</td></tr>
				<tr><td>
					<i>A Novel Multi-Modal Rehabilitation Monitoring Over Human Motion Intention Recognition.</i> Kamal, S., Alshehri, M., Alqahtani, Y., Alshahrani, A., Almujally, N., Jalal, A.*, Liu, H.* (2025). Frontiers in Bioengineering and Biotechnology, 13:1568690. ISSN: 2296-4185.<br>
					<a href="https://scholar.google.com/scholar?q=A+Novel+Multi-Modal+Rehabilitation+Monitoring+Over+Human+Motion+Intention+Recognition" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/A-novel-multi-modal-rehabilitation-monitoring-over-Kamal-Alshehri/9fdbdf8f625447d74de4dd33a4693feca6794ed0" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/393775118_A_novel_multi-modal_rehabilitation_monitoring_over_human_motion_intention_recognition" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/bioengineering-and-biotechnology/articles/10.3389/fbioe.2025.1568690" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fbioe.2025.1568690" target="_blank" rel="noopener noreferrer">10.3389/fbioe.2025.1568690</a>
				</td></tr>
				<tr><td>
					<i>Intelligent Biosensors for Human Movement Rehabilitation and Intention Recognition.</i> Rafiq, M., Almujally, N., Algarni, A., Jalal, A.*, and Liu, H.* (2025). Frontiers in Bioengineering and Biotechnology, 13:1558529. ISSN: 2296-4185.<br>
					<a href=https://scholar.google.com/scholar?q=Intelligent+Biosensors+for+Human+Movement+Rehabilitation+and+Intention+Recognition" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Intelligent-biosensors-for-human-movement-and-Rafiq-Almujally/a59b0c596eb401f22ecc53c0d519d1c67d0680f4" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/393557563_Intelligent_biosensors_for_human_movement_rehabilitation_and_intention_recognition" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/bioengineering-and-biotechnology/articles/10.3389/fbioe.2025.1558529" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fbioe.2025.1558529" target="_blank" rel="noopener noreferrer">10.3389/fbioe.2025.1558529</a>
				</td></tr>
				<tr><td>
					<i>A Novel Deep Learner for Human Behavior Prediction over Public Video Surveillance.</i> Alabdullah, B, Fatima, B. B., Alhasson, H. F., Alshehri, M., AlQahtani, Y., Albhassabi, N., and Liu, H.* (2025). IEEE Access, 13:108718-108731.<br>
					<a href="https://scholar.google.com/scholar?q=A+Novel+Deep+Learner+for+Human+Behavior+Prediction+over+Public+Video+Surveillance" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/A-Novel-Deep-Learner-for-Human-Behavior-Prediction-Alabdullah-Fatima/89ed33d1b67479759961f8cb573781afa244cfed" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/392742279_A_Novel_Deep_Learner_for_Human_Behavior_Prediction_over_Public_Video_Surveillance" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/11036735" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/ACCESS.2025.3579749" target="_blank" rel="noopener noreferrer">10.1109/ACCESS.2025.3579749</a>
				</td></tr>
				<tr><td>
					<i>Wearable Sensors-based Assistive Technologies for Patient Health Monitoring.</i> Almujally, N. A., Khan, D., Al Mudawi, N., Alonazi, M., Algarni, A., Jalal, A.*, and Liu, H.* (2025). Frontiers in Bioengineering and Biotechnology, 13:1437877. ISSN: 2296-4185.<br>
					<a href=https://scholar.google.com/scholar?q=Wearable+Sensors-based+Assistive+Technologies+for+Patient+Health+Monitoring" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Wearable-sensors-based-assistive-technologies-for-Almujally-Khan/e12e777b87213cc001119eb078eff21ff0b4583f" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/392323004_Wearable_sensors-based_assistive_technologies_for_patient_health_monitoring" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/bioengineering-and-biotechnology/articles/10.3389/fbioe.2025.1437877" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fbioe.2025.1437877" target="_blank" rel="noopener noreferrer">10.3389/fbioe.2025.1437877</a>
				</td></tr>
				<tr><td>
					<i>Multimodal Intelligent Biosensors Framework for Fall Disease Detection and Healthcare Monitoring.</i> Abro, I. A., Alharbi, S. S., Alshammari, N. S., Algarni, S., Almujally, N. A., Jalal, A.*, and Liu, H.* (2025). Frontiers in Bioengineering and Biotechnology, 13:1544968. ISSN: 2296-4185.<br>
					<a href="https://scholar.google.com/scholar?q=Multimodal+Intelligent+Biosensors+Framework+for+Fall+Disease+Detection+and+Healthcare+Monitoring" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Multimodal-intelligent-biosensors-framework-for-and-Abro-Alharbi/8d1b34e034e648299ac16f29be1993d2dc68409a" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/392695110_Multimodal_Intelligent_Biosensors_Framework_for_Fall_Disease_Detection_and_Healthcare_Monitoring" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/bioengineering-and-biotechnology/articles/10.3389/fbioe.2025.1544968" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
			 		&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fbioe.2025.1544968" target="_blank" rel="noopener noreferrer">10.3389/fbioe.2025.1544968</a>
				</td></tr>
				<tr><td>   
					<i>Towards Human Modeling for Human-Robot Collaboration and Digital Twins in Industrial Environments: Research Status, Prospects, and Challenges.</i> Xia, G.*, Ghrairi, Z., Wuest, T., Hribernik, K., Heuermann, A., Liu, F., Liu, H., Thoben, K. (2025). Robotics and Computer-Integrated Manufacturing, 95:103043.<br>
					<a href="https://scholar.google.com/scholar?q=Towards+Human+Modeling+for+Human-Robot+Collaboration+and+Digital+Twins+in+Industrial+Environments%3A+Research+Status%2C+Prospects%2C+and+Challenges" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Towards-Human-Modeling-for-Human-Robot-and-Digital-Xia-Ghrairi/e0045bf565ed6c1bafb14d880478802d6dedd835" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/391344160_Towards_Human_Modeling_for_Human-Robot_Collaboration_and_Digital_Twins_in_Industrial_Environments_Research_Status_Prospects_and_Challenges" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0736584525000973" target="_blank" rel="noopener noreferrer"><img src="images/Elsevier.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1016/j.rcim.2025.103043" target="_blank" rel="noopener noreferrer">10.1016/j.rcim.2025.103043</a>
				</td></tr>
				<tr><td>
					<i>Advanced Leaf Classification using Multi-Layer Perceptron for Smart Crop Management.</i> Mumtaz S., Alshehri M., AlQahtani Y., Alshahrani A., Alabdullah, B., Alhasson, H. F., Jalal, A.*, and Liu, H.* (2025). IEEE Access, 13:105579-105589.<br>
					<a href="https://scholar.google.com/scholar?q=Advanced+Leaf+Classification+using+Multi-Layer+Perceptron+for+Smart+Crop+Management" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Advanced-Leaf-Classification-using-Multi-Layer-for-Mumtaz-Alshehri/44e3d0856aa58ec611b26a35cd48fc79a955615d" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/392040833_Advanced_Leaf_Classification_using_Multi-Layer_Perceptron_for_Smart_Crop_Management" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/11014056" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
			 		&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/ACCESS.2025.3572985" target="_blank" rel="noopener noreferrer">10.1109/ACCESS.2025.3572985</a>
				</td></tr>
				<tr><td>
					<i>Multi-Modal Remote Sensory Learning for Multi-Objects over Autonomous Devices.</i> Naseer, A., Almudawi, N., Aljuaid, H., Alazeb, A., Alqahtani, Y., Algarni, A., Jalal, A.*, and Liu, H.* (2025). Frontiers in Bioengineering and Biotechnology, 13:1430222. ISSN: 2296-4185.<br>
					<a href="https://scholar.google.com/scholar?q=Multi-Modal+Remote+Sensory+Learning+for+Multi-Objects+over+Autonomous+Devices" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Multi-modal-remote-sensory-learning-for-over-Naseer-Almudawi/afb79f30ca3fcebfaca42666ad3a5e1f10d4b99d" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/391929938_Multi-modal_remote_sensory_learning_for_multi-objects_over_autonomous_devices" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fbioe.2025.1430222" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fbioe.2025.1430222" target="_blank" rel="noopener noreferrer">10.3389/fbioe.2025.1430222</a>
				</td></tr>
				<tr><td>
					<i>A Deep Learning Framework for Healthy LifeStyle Monitoring and Outdoor Localization.</i> Rafiq, M., Albhassabi, N., Alhasson, H. F., AlHammadi, D. A., Alshehri, M., Jalal, A.*, and Liu, H.* (2025). IEEE Access, 13:118695-118705.<br>
					<a href="https://scholar.google.com/scholar?q=A+Deep+Learning+Framework+for+Healthy+LifeStyle+Monitoring+and+Outdoor+Localization" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/A-Deep-Learning-Framework-for-Healthy-LifeStyle-and-Rafiq-Alshammari/e0cda5a47821a8322c3994f5ee7fa0779e685ac1" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/392155986_A_Deep_Learning_Framework_for_Healthy_LifeStyle_Monitoring_and_Outdoor_Localization" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/11015451" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
			 		&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/ACCESS.2025.3573439" target="_blank" rel="noopener noreferrer">10.1109/ACCESS.2025.3573439</a>
				</td></tr>
				<tr><td>
					<i>Thermo-Environ-Economic Analysis of a Novel Solar-Assisted Heat Pump System; Comparison with Conventional Single Stage and Cascaded Air Source Heat Pumps.</i> Afshin, M. M. and Liu, H.* (2025). Energy, 322:135647. Elsevier. ISSN: 0360-5442.<br>
					<a href="https://scholar.google.com/scholar?q=Thermo-environ-economic+analysis+of+a+novel+solar-assisted+heat+pump+system%3B+comparison+with+conventional+single+stage+and+cascaded+air+source+heat+pumps" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Thermo-environ-economic-analysis-of-a-novel-heat-Manesh-Liu/b5444342a944baf5bc5b34741fc480f97b40cf61" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/389881579_Thermo-environ-economic_analysis_of_a_novel_solar-assisted_heat_pump_system_comparison_with_conventional_single_stage_and_cascaded_air_source_heat_pumps" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0360544225012897" target="_blank" rel="noopener noreferrer"><img src="images/Elsevier.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1016/j.energy.2025.135647" target="_blank" rel="noopener noreferrer">10.1016/j.energy.2025.135647</a>
				</td></tr>
				<tr><td>
					<i>Vision Sensor for Automatic Recognition of Human Activities via Hybrid Features and Multi-Class Support Vector Machine.</i> Kamal, S., Alhasson, H. F., Alnusayri, M., Alatiyyah, M., Aljuaid, H., Jalal, A.*, and Liu, H.* (2025). Sensors, 25(1):200.<br>
					<a href="https://scholar.google.com/scholar?q=Vision+Sensor+for+Automatic+Recognition+of+Human+Activities+via+Hybrid+Features+and+Multi-Class+Support+Vector+Machine" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Vision-Sensor-for-Automatic-Recognition-of-Human-Kamal-Alhasson/0c322f268f8d28076a15614ba4d8a7de7575e6e6" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/387642119_Vision_Sensor_for_Automatic_Recognition_of_Human_Activities_via_Hybrid_Features_and_Multi-Class_Support_Vector_Machine" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/1424-8220/25/1/200" target="_blank" rel="noopener noreferrer"><img src="images/Sensors.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/s25010200" target="_blank" rel="noopener noreferrer">10.3390/s25010200</a>
				</td></tr>
				<tr><td>
					<i>Enhancing Smart Building Performance with Waste Heat Recovery: Supply-Side Management, Demand Reduction, and Peak Shaving via Advanced Control Systems.</i> Liu, H., Du, Z., Xue, T., and Jiang, T.* (2025). Energy and Buildings, 327:115070. Elsevier. ISSN: 0378-7788.<br>
					<a href="https://scholar.google.com/scholar?q=Enhancing+Smart+Building+Performance+with+Waste+Heat+Recovery%3A+Supply-Side+Management%2C+Demand+Reduction%2C+and+Peak+Shaving+via+Advanced+Control+Systems" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Enhancing-smart-building-performance-with-waste-and-Liu-Du/a05fa014656b2ccd586a50ae481d5e3ebd23aea6" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/385884059_Enhancing_smart_building_performance_with_waste_heat_recovery_Supply-side_management_demand_reduction_and_peak_shaving_via_advanced_control_systems" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0378778824011861" target="_blank" rel="noopener noreferrer"><img src="images/Elsevier.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1016/j.enbuild.2024.115070" target="_blank" rel="noopener noreferrer">10.1016/j.enbuild.2024.115070</a>
				</td></tr>
				<tr><td>
					<i>Leaf Classification for Sustainable Agriculture and In-Depth Species Analysis.</i> Mumtaz, S., Alhammadi, D. A., Jalal, A.*, and Liu, H.* (2025). IEEE Access, 13:17043-17053.<br>
					<a href="https://scholar.google.com/scholar?q=Leaf+Classification+for+Sustainable+Agriculture+and+In-Depth+Species+Analysis" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Leaf-Classification-for-Sustainable-Agriculture-and-Mumtaz-Algamdi/990208478f7bf9cbb33872a610a3140e5b9a44c0" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/387816363_Leaf_Classification_for_Sustainable_Agriculture_and_In-Depth_Species_Analysis" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/10830505" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/ACCESS.2025.3526888" target="_blank" rel="noopener noreferrer">10.1109/ACCESS.2025.3526888</a>
				</td></tr>
				<tr><td>
				<i>IoT Powered RNN for Improved Human Activity Recognition with Enhanced Localization and Classification.</i> Mudawi, N. A., Azmat, U., Alazeb, A., Alhasson, H. F., Alabdullah, B., Rahman, H., Liu, H.*, and Jalal, A.* (2025). Scientific Reports, 15:10328. Springer Nature. ISSN: 2045-2322.<br>
					<a href="https://scholar.google.com/scholar?q=IoT+Powered+RNN+for+Improved+Human+Activity+Recognition+with+Enhanced+Localization+and+Classification" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/IoT-powered-RNN-for-improved-human-activity-with-Mudawi-Azmat/17133d2e3a7a373249700481dbe04c08d6b051e1" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/390178040_IoT_powered_RNN_for_improved_human_activity_recognition_with_enhanced_localization_and_classification" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.nature.com/articles/s41598-025-94689-5" target="_blank" rel="noopener noreferrer"><img src="images/Nature.svg" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/article/10.1038/s41598-025-94689-5" target="_blank" rel="noopener noreferrer"><img src="images/SpringerLink.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1038/s41598-025-94689-5" target="_blank" rel="noopener noreferrer">10.1038/s41598-025-94689-5</a>
				</td></tr>
				<tr><td>
					<i>Attention-Driven Emotion Recognition in EEG: A Transformer-Based Approach with Cross-Dataset Fine-Tuning.</i> Ghous, G., Najam, S., Alshehri, M., Alshahrani, A., AlQahtani, Y., Jalal, A.*, and Liu, H.* (2025). IEEE Access, 13:69369-69394.<br>
					<a href="https://scholar.google.com/scholar?q=Attention-Driven+Emotion+Recognition+in+EEG%3A+A+Transformer-Based+Approach+with+Cross-Dataset+Fine-Tuning" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Attention-Driven-Emotion-Recognition-in-EEG%3A-A-with-Ghous-Najam/98012d1f5cf001b0a34d2646f2f1b27792ea545e" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/390825580_Attention-Driven_Emotion_Recognition_in_EEG_A_Transformer-Based_Approach_with_Cross-Dataset_Fine-Tuning" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/10965678" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/ACCESS.2025.3561137" target="_blank" rel="noopener noreferrer">10.1109/ACCESS.2025.3561137</a>
				</td></tr>
				<tr><td>
					<i>Unmanned Aerial Vehicle based Multi-Person Detection via Deep Neural Network Models.</i> Alshehri, M., Zahoor, L., Alqahtani, Y., Alshahrani, A., Al-Hammadi, D. A.,Jalal, A.*, and Liu, H.* (2025). Frontiers in Neurorobotics, 19:1582995. ISSN: 1662-5218.<br>
					<a href="https://scholar.google.com/scholar?q=Unmanned+Aerial+Vehicle+based+wMulti-Person+Detection+via+Deep+Neural+Network+Models" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Unmanned-aerial-vehicle-based-multi-person-via-deep-Alshehri-Zahoor/dcd9ed949031563c146aad83b0484d0a56d1b1b6" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/390853268_Unmanned_aerial_vehicle_based_multi-person_detection_via_deep_neural_network_models" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2025.1582995" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fnbot.2025.1582995" target="_blank" rel="noopener noreferrer">10.3389/fnbot.2025.1582995</a>
				</td></tr>
				<tr><td>
					<i>Advanced Biosignal-RGB Fusion with Adaptive Neurofuzzy Classification for High-Precision Action Recognition.</i> Abro, I. A., Alhasson, H. F., Alharbi, S. S., Alatiyyah, M., AlHammadi, D. A., Jalal, A.*, and Liu, H.* (2025). IEEE Access, 13:57287-57310.<br>
					<a href="https://scholar.google.com/scholar?q=Advanced+Biosignal-RGB+Fusion+with+Adaptive+Neurofuzzy+Classification+for+High-Precision+Action+Recognition" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Advanced-Biosignal-RGB-Fusion-with-Adaptive-for-Abro-Alhasson/09fd858fbd11943b768d983aac0c2e4bcf24e38a" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/390087724_Advanced_Biosignal-RGB_Fusion_with_Adaptive_Neurofuzzy_Classification_for_High-Precision_Action_Recognition" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/10935610" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/ACCESS.2025.3553196" target="_blank" rel="noopener noreferrer">10.1109/ACCESS.2025.3553196</a>
				</td></tr>
				<tr><td>
					<i>Evaluating the Potential of Microdroplet Flow in Two-Phase Biocatalysis: A Systematic Study.</i> Xiang, L., Solarczek, J., Krajka, V., Liu, H., Ahlborn, L., Schallmey, A., and Constantinou, I.* (2025). ACS Applied Materials & Interfaces, 17(3):4776–4787. ACS Publications.<br>
					<a href="https://scholar.google.com/scholar?q=Evaluating+the+Potential+of+Microdroplet+Flow+in+Two-Phase+Biocatalysis%3A+A+Systematic+Study" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Evaluating-the-Potential-of-Microdroplet-Flow-in-A-Xiang-Solarczek/9f833e3e56a489fdfe312eb4fc36c05961cb59d8" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href=https://www.researchgate.net/publication/387894091_Evaluating_the_Potential_of_Microdroplet_Flow_in_Two-Phase_Biocatalysis_A_Systematic_Study" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://pubs.acs.org/doi/10.1021/acsami.4c15647" target="_blank" rel="noopener noreferrer"><img src="images/ACS.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1021/acsami.4c15647" target="_blank" rel="noopener noreferrer">10.1021/acsami.4c15647</a>
				</td></tr>
				<tr><td>
					<i>A Hybrid Approach for Sports Activity Recognition Using Key Body Descriptors and Hybrid Deep Learning Classifier.</i> Tayyab M., Alateyah, S. A., Alnusayri, M., Alatiyyah, M., AlHammadi, D. A., Jalal, A.*, Liu, H.* (2025). Sensors, 25(2):441.<br>
					<a href="https://scholar.google.com/scholar?q=A+Hybrid+Approach+for+Sports+Activity+Recognition+using+Key+Body+Descriptors+and+Hybrid+Deep+Learning+Classifier" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/A-Hybrid-Approach-for-Sports-Activity-Recognition-Tayyab-Alateyah/cf5f114da0361c7337cd88563760b01e6d0c7915" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/387968617_A_Hybrid_Approach_for_Sports_Activity_Recognition_Using_Key_Body_Descriptors_and_Hybrid_Deep_Learning_Classifier" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/1424-8220/25/2/441" target="_blank" rel="noopener noreferrer"><img src="images/Sensors.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/s25020441" target="_blank" rel="noopener noreferrer">10.3390/s25020441</a>
				</td></tr>
				<tr><td>
					<i>An Integrated Artificial Intelligence-Driven Approach to Multi-Criteria Optimization of Building Energy Efficiency and Occupants' Comfort: A Case Study.</i> Liu, H., Du, Z., Xue, T., and Jiang, T.* (2025). Journal of Building Engineering, 103:111944. Elsevier. ISSN: 2352-7102.<br>
					<a href="https://scholar.google.com/scholar?q=An+Integrated+Artificial+Intelligence-Driven+Approach+to+Multi-Criteria+Optimization+of+Building+Energy+Efficiency+and+Occupants%27+Comfort%3A+A+Case+Study" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/An-integrated-artificial-intelligence-driven-to-of-Liu-Du/69b8567a80b5853b541dc1902db05782fac2209f" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/388745151_An_integrated_artificial_intelligence-driven_approach_to_multi-criteria_optimization_of_building_energy_efficiency_and_occupants'_comfort_A_case_study" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S2352710225001809" target="_blank" rel="noopener noreferrer"><img src="images/Elsevier.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1016/j.jobe.2025.111944" target="_blank" rel="noopener noreferrer">10.1016/j.jobe.2025.111944
					</a>
				</td></tr>
				<tr><td>
					<i>Human Pose Estimation and Event Recognition via Feature Extraction and Neuro-Fuzzy Classifier.</i> Hanzla, M., Alshammari, N. S., Alharbi, S. S., Wahid, W., Almujally, N. A., Jalal, A.*, Liu, H.* (2025). IEEE Access, 13:37328-37343.<br>
					<a href="https://scholar.google.com/scholar?q=Human+Pose+Estimation+and+Event+Recognition+via+Feature+Extraction+and+Neuro-Fuzzy+Classifier" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Human-Pose-Estimation-and-Event-Recognition-via-and-Hanzla-Alshammari/37c8b39cbe842726388b934b55f9eea8e6745e62" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/388699517_Human_Pose_Estimation_and_Event_Recognition_via_Feature_Extraction_and_Neuro-Fuzzy_Classifier" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/10870209" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/ACCESS.2025.3538691" target="_blank" rel="noopener noreferrer">10.1109/ACCESS.2025.3538691</a>
				</td></tr>	
				<tr><td>
					<i>Drone-based Public Surveillance using 3D Point Clouds and Neuro-Fuzzy Classifier.</i> Abbas, Y., Alarfaj, A. A., Alabdulqader, E. A., Algarni, A., Jalal, A., Liu, H.* (2025). Computer, Materials & Continua, 82(3):4759-4776. Tech Science Press. ISSN: 1546-2218.<br>
					<a href="https://scholar.google.com/scholar?q=Drone-based+Public+Surveillance+using+3D+Point+Clouds+and+Neuro-Fuzzy+Classifier" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Drone-Based-Public-Surveillance-Using-3D-Point-and-Abbas-Alarfaj/ed518104b3fc77f3b26fe09e17c8168da988c416" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/389182487_Drone-Based_Public_Surveillance_Using_3D_Point_Clouds_and_Neuro-Fuzzy_Classifier" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.techscience.com/cmc/online/detail/22607" target="_blank" rel="noopener noreferrer"><img src="images/Tech_Science_Press.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.32604/cmc.2025.059224" target="_blank" rel="noopener noreferrer">10.32604/cmc.2025.059224</a>
				</td></tr>
				<tr><td>
					<i>Remote Sensing Surveillance using Multilevel Feature Fusion and Deep Neural Network.</i> Zahoor, L., Alhasson, H. F., Alnusayri, M., Alatiyyah, M., AlHammadi, D. A., Jalal, A.*, Liu, H.* (2025). IEEE Access, 13:38282-38300.<br>
					<a href="https://scholar.google.com/scholar?q=Remote+Sensing+Surveillance+using+Multilevel+Feature+Fusion+and+Deep+Neural+Network" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Remote-Sensing-Surveillance-using-Multilevel-Fusion-Zahoor-Alhasson/cd8f9db2848e67b19d87869a2a0d7f9132639bc0" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/389007840_Remote_Sensing_Surveillance_using_Multilevel_Feature_Fusion_and_Deep_Neural_Network" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/10890997" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/ACCESS.2025.3542435" target="_blank" rel="noopener noreferrer">10.1109/ACCESS.2025.3542435</a>
				</td></tr>
				<tr><td>
					<i>TPRO-NET: An EEG-Based Emotion Recognition Method Reflecting Subtle Changes in Emotion.</i> Zhang, X., Cheng, X.*, and Liu, H.* (2024). Scientific Reports, 14:13491. Springer Nature. ISSN: 2045-2322.<br>
					<a href="https://scholar.google.com/scholar?q=TPRO-NET%3A+An+EEG-Based+Emotion+Recognition+Method+Feflecting+Subtle+Changes+in+Emotion" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
				    &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/TPRO-NET%3A-an-EEG-based-emotion-recognition-method-Zhang-Cheng/5b1c1fa371e436215731026398119a2882a13cb4" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/381352163_TPRO-NET_an_EEG-based_emotion_recognition_method_reflecting_subtle_changes_in_emotion" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.nature.com/articles/s41598-024-62990-4" target="_blank" rel="noopener noreferrer"><img src="images/Nature.svg" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/article/10.1038/s41598-024-62990-4" target="_blank" rel="noopener noreferrer"><img src="images/SpringerLink.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1038/s41598-024-62990-4" target="_blank" rel="noopener noreferrer">10.1038/s41598-024-62990-4</a>
				</td></tr>
				<tr><td>
					<i>Semantic Segmentation and YOLO Detector over Aerial Vehicle Images.</i> Quresh, A. M., Butt, A. H., Alazeb, A., Al Mudawi, N., Alonazi, M., Almujally, N. A., Jalal, A., and Liu, H.* (2024).  Computer, Materials & Continua, 80(2):3315-3332. Tech Science Press. ISSN: 1546-2218.<br>
					<a href="https://scholar.google.com/scholar?q=Semantic+Segmentation+and+YOLO+Detector+over+Aerial+Vehicle+Images" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Semantic-Segmentation-and-YOLO-Detector-over-Aerial-Qureshi/280b84a3d90003c24c1d6c032227387d23fcea2b" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/383167511_Semantic_Segmentation_and_YOLO_Detector_over_Aerial_Vehicle_Images" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.techscience.com/cmc/v80n2/57623" target="_blank" rel="noopener noreferrer"><img src="images/Tech_Science_Press.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.32604/cmc.2024.052582" target="_blank" rel="noopener noreferrer">10.32604/cmc.2024.052582</a>
				</td></tr>
				<tr><td>
					<i>Template-Based Synergy Extrapolation Analysis for Prediction of Muscle Excitations.</i> Li, K., Wang, D., Chen, Z, Guo, D, Pan, S., Liu, H., Zhou, C.*, and Ye, X.* (2024). Physiological Measurement, 45(9):095016. IOPScience.<br>
					<a href="https://scholar.google.com/scholar?q=Template-Based+Synergy+Extrapolation+Analysis+for+Prediction+of+Muscle+Excitations" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Template-based-synergy-extrapolation-analysis-for-Li-Wang/4a53818d9bdc84e8f3f6926ce67f6240f3a27867" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/383771707_Template-based_synergy_extrapolation_analysis_for_prediction_of_muscle_excitations" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://iopscience.iop.org/article/10.1088/1361-6579/ad7776" target="_blank" rel="noopener noreferrer"><img src="images/IOP.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1088/1361-6579/ad7776" target="_blank" rel="noopener noreferrer">10.1088/1361-6579/ad7776</a>
				</td></tr>
				<tr><td>
					<i>A Robust Multimodal Detection: Physical Exercise Monitoring in Long-Term Care Environments.</i> Al Mudawi, N., Batool M., Alazeb A., Alqahtani Y., Almujally, N. A., Algarni A., Jalal, A.*, and Liu, H.* (2024). Frontiers in Bioengineering and Biotechnology, 12:1398291. ISSN: 2296-4185.<br>
					<a href="https://scholar.google.com/scholar?q=A+Robust+Multimodal+Detection%3A+Physical+Exercise+Monitoring+in+Long-Term+Care+Environments" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/A-robust-multimodal-detection-system%3A-physical-in-Mudawi-Batool/6cc77f006d73ee532d47447e5dfceff91555e57d" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/382973776_A_robust_multimodal_detection_system_Physical_exercise_monitoring_in_long-term_care_environments" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/bioengineering-and-biotechnology/articles/10.3389/fbioe.2024.1398291" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fbioe.2024.1398291" target="_blank" rel="noopener noreferrer">10.3389/fbioe.2024.1398291</a>
				</td></tr>
				<tr><td>
					<i>Robust Human Interaction Recognition Using Extended Kalman Filter.</i> Jalal, A., Bukht, T. F. N., Alazeb, A., Al Mudawi, N., Alabdullah, B, Alnowaiser, K., and Liu, H.* (2024). Computer, Materials & Continua, 81(2):2987-3002. Tech Science Press. ISSN: 1546-2218.<br>
					<a href="https://scholar.google.com/scholar?q=Robust+Human+Interaction+Recognition+Using+Extended+Kalman+Filter" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Robust-Human-Interaction-Recognition-Using-Extended-Bukht-Alazeb/512542deb4c95d6687f86ae8eb18ecd55cb7faba" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/385930385_Robust_Human_Interaction_Recognition_Using_Extended_Kalman_Filter" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.techscience.com/cmc/v81n2/58626" target="_blank" rel="noopener noreferrer"><img src="images/Tech_Science_Press.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.32604/cmc.2024.053547" target="_blank" rel="noopener noreferrer">10.32604/cmc.2024.053547</a>
				</td></tr>
				<tr><td>
					<i>Innovative Healthcare Solutions: Robust Hand Gesture Recognition of Daily Life Routines Using 1D CNN.</i> Al Mudawi, N., Ansar, H., Alazeb, A., Aljuaid, H., Alqahtani, Y., Algarni, A., Jalal, A.*, and Liu, H.* (2024). Frontiers in Bioengineering and Biotechnology, 12:1401803. ISSN: 2296-4185.<br>
					<a href="https://scholar.google.com/scholar?q=Innovative+healthcare+solutions%3A+robust+hand+gesture+recognition+of+daily+life+routines+using+1D+CNN" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Innovative-healthcare-solutions%3A-robust-hand-of-1D-Mudawi-Ansar/6ea62f04e69a0ceb72a0a6bce57fa8f321f476e3" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/382741631_Innovative_healthcare_solutions_robust_hand_gesture_recognition_of_daily_life_routines_using_1D_CNN" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/bioengineering-and-biotechnology/articles/10.3389/fbioe.2024.1401803" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fbioe.2024.1401803" target="_blank" rel="noopener noreferrer">10.3389/fbioe.2024.1401803</a>
				</td></tr>
				<tr><td>
					<i>Human Activity Recognition, Monitoring, and Analysis Facilitated by Novel and Widespread Applications of Sensors.</i> Liu, H.*, Gamboa, H., and Schultz, T. (2024). Sensors, 24(15):5250.<br>
					<a href="https://scholar.google.com/scholar?q=Human+Activity+Recognition%2C+Monitoring%2C+and+Analysis+Facilitated+by+Novel+and+Widespread+Applications+of+Sensors" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Human-Activity-Recognition%2C-Monitoring%2C-and-by-and-Liu-Gamboa/0b502fb4f7a4af87b9626e147c8fffe99c97eebf" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/383117809_Human_Activity_Recognition_Monitoring_and_Analysis_Facilitated_by_Novel_and_Widespread_Applications_of_Sensors" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/1424-8220/24/16/5250" target="_blank" rel="noopener noreferrer"><img src="images/Sensors.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/s24165250" target="_blank" rel="noopener noreferrer">10.3390/s24165250</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/books/book/10390" target="_blank" rel="noopener noreferrer"><img src="images/Book.png" height="15"></a>
				</td></tr>
				<tr><td>
					<i>Multi-Modal Remote Perception Learning for Object Sensory Data.</i> Almujally, N. A., Rafique, A. A., Mudawi, N. A., Alazeb, A., Alonazi, M., Algarni, A., Jalal, A.*, and Liu, H.* (2024). Frontiers in Neurorobotics, 18:1427786. ISSN: 1662-5218.<br>
					<a href="https://scholar.google.com/scholar?q=Multi-modal+remote+perception+learning+for+object+sensory+data" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Multi-modal-remote-perception-learning-for-object-Almujally-Rafique/1b8febbf47bf63a16aa2625dd731461471df6ff1" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>		
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/384147762_Multi-modal_remote_perception_learning_for_object_sensory_data" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1427786" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fnbot.2024.1427786" target="_blank" rel="noopener noreferrer">10.3389/fnbot.2024.1427786</a>
				<tr><td>
					<i>Enhanced Data Mining and Visualization of Sensory-Graph-Modeled Datasets through Summarization.</i> Hashmi, S. J., Alabdullah, B., Al Mudawi, N., Algarni, A., Jalal, A.*, and Liu, H.* (2024). Sensors, 24(14):4554.<br>
					<a href="https://scholar.google.com/scholar?q=Enhanced+Data+Mining+and+Visualization+of+Sensory-Graph-Modeled+Datasets+through+Summarization" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Enhanced-Data-Mining-and-Visualization-of-Datasets-Hashmi-Alabdullah/18e0faa9b8fe1aefadbc5cbfd0744e50a88d8715" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/382288996_Enhanced_Data_Mining_and_Visualization_of_Sensory-Graph-Modeled_Datasets_through_Summarization" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/1424-8220/24/14/4554" target="_blank" rel="noopener noreferrer"><img src="images/Sensors.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/s24144554" target="_blank" rel="noopener noreferrer">10.3390/s24144554</a>
				</td></tr>
				<tr><td>
					<i>Understanding Naturalistic Facial Expressions with Deep Learning and Multimodal Large Language Models.</i> Bian, Y., Küster, D., Liu H., and Krumhuber, E. G.* (2024). Sensors, 24(1):126.<br>
					<a href="https://scholar.google.com/scholar?q=Understanding+Naturalistic+Facial+Expressions+with+Deep+Learning+and+Multimodal+Large+Language+Models" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Understanding-Naturalistic-Facial-Expressions-with-Bian-K%C3%BCster/6dbb5a3f98625d7d090b37ae36dbf3e64d1f1d0e" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/376840648_Understanding_Naturalistic_Facial_Expressions_with_Deep_Learning_and_Multimodal_Large_Language_Models" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/1424-8220/24/1/126" target="_blank" rel="noopener noreferrer"><img src="images/Sensors.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/s24010126" target="_blank" rel="noopener noreferrer">10.3390/s24010126</a>
				</td></tr>
				<tr><td>
					<i>Cardiorespiratory Response to Workload Volume and Ergonomic Risk: Automotive Assembly Line Operators’ Adaptations.</i> Furk, D., Silva, L.*, Dias, M., Fujão, C., Probst, P., Liu, H., Gamboa, H. (2024). Applied Sciences, 14(9):3921.<br>
					<a href="https://scholar.google.com/scholar?q=Cardiorespiratory+Response+to+Workload+Volume+and+Ergonomic+Risk%3A+Automotive+Assembly+Line+Operators’+Adaptations" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Cardiorespiratory-Response-to-Workload-Volume-and-Furk-Silva/84e55a2464c47b2017c65a39a28aa4427dbe0799" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/380382468_Cardiorespiratory_Response_to_Workload_Volume_and_Ergonomic_Risk_Automotive_Assembly_Line_Operators'_Adaptations" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/2076-3417/14/9/3921" target="_blank" rel="noopener noreferrer"><img src="images/Applied_Sciences.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/app14093921" target="_blank" rel="noopener noreferrer">10.3390/app14093921</a>
				</td></tr>
				<tr><td>
					<i>Hybrid Modeling on Reconstitution of Continuous Arterial Blood Pressure Using Finger Photoplethysmography.</i> Shi, W., Zhou, C., Zhang, Y., Li, K., Ren, X., Liu, H.*, and Ye, X.* (2023). Biomedical Signal Processing and Control, 85:104972. Elsevier. ISSN: 1746-8094.<br>
					<a href="https://scholar.google.com/scholar?q=Hybrid+modeling+on+reconstitution+of+continuous+arterial+blood+pressure+using+finger+photoplethysmography" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Hybrid-modeling-on-reconstitution-of-continuous/72ac3930e09b7ec7008ea029cba28ae1ba709e54" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/370529977_Hybrid_Modeling_on_Reconstitution_of_Continuous_Arterial_Blood_Pressure_Using_Finger_Photoplethysmography" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.sciencedirect.com/science/article/abs/pii/S1746809423004056" target="_blank" rel="noopener noreferrer"><img src="images/Elsevier.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1016/j.bspc.2023.104972" target="_blank" rel="noopener noreferrer">10.1016/j.bspc.2023.104972</a>
				</td></tr>
				<tr><td>
					<i>Sensor-Based Human Activity and Behavior Research: Where Advanced Sensing and Recognition Technologies Meet.</i> Liu, H.*, Gamboa, H., and Schultz, T. (2023). Sensors, 23(1):125.<br>
					<a href="https://scholar.google.com/scholar?q=sensor-based+human+activity+and+behavior+research" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Sensor-Based-Human-Activity-and-Behavior-Research%3A-Liu-Gamboa/d775d7f0105130e9f47ef95fd73d46243e385848" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/366563962_Sensor-Based_Human_Activity_and_Behavior_Research_Where_Advanced_Sensing_and_Recognition_Technologies_Meet" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/1424-8220/23/1/125" target="_blank" rel="noopener noreferrer"><img src="images/Sensors.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/s23010125" target="_blank" rel="noopener noreferrer">10.3390/s23010125</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/books/book/7447" target="_blank" rel="noopener noreferrer"><img src="images/Book.png" height="15"></a>
				</td></tr>
				<tr><td>
					<i>Feature-Based Information Retrieval of Multimodal Biosignals with a Self-Similarity Matrix: Focus on Automatic Segmentation.</i> Rodrigues, J.*., Liu, H.* (co-first), Folgado, D., Belo, D., Schultz, T., and Gamboa, H.* (2022). Biosensors, 12(12):1182. MDPI. ISSN: 2079-6374.<br>
					<a href="https://scholar.google.com/scholar?q=Feature-Based+Information+Retrieval+of+Multimodal+Biosignals+with+a+Self-Similarity+Matrix%3A+Focus+on+Automatic+Segmentation" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Feature-Based-Information-Retrieval-of-Multimodal-a-Rodrigues-Liu/c824d095fbf7d9afdde8c36909fc70eb569f15c3" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/366410597_Feature-Based_Information_Retrieval_of_Multimodal_Biosignals_with_a_Self-Similarity_Matrix_Focus_on_Automatic_Segmentation" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/2079-6374/12/12/1182" target="_blank" rel="noopener noreferrer"><img src="images/Biosensors.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/bios12121182" target="_blank" rel="noopener noreferrer">10.3390/bios12121182</a>
				</td></tr>
				<tr><td>
					<i>Bell Shape Embodying Zhongyong: The Pitch Histogram of Traditional Chinese Anhemitonic Pentatonic Folk Songs.</i> Liu, H.*, Jiang, K., Gamboa, H., Xue, T., and Schultz, T. (2022). Applied Sciences, 12(16):8343.<br>
					<a href="https://scholar.google.com/scholar?q=Bell+Shape+Embodying+Zhongyong%3A+The+Pitch+Histogram+of+Traditional+Chinese+Anhemitonic+Pentatonic+Folk+Songs" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Bell-Shape-Embodying-Zhongyong%3A-The-Pitch-Histogram-Liu-Jiang/db1523092f1e528af3ce41385843e862b74f0ff2" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/362837075_Bell_Shape_Embodying_Zhongyong_The_Pitch_Histogram_of_Traditional_Chinese_Anhemitonic_Pentatonic_Folk_Songs" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.mdpi.com/2076-3417/12/16/8343" target="_blank" rel="noopener noreferrer"><img src="images/Applied_Sciences.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://novaresearch.unl.pt/en/publications/bell-shape-embodying-zhongyong-the-pitch-histogram-of-traditional" target="_blank" rel="noopener noreferrer"><img src="images/Nova.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://run.unl.pt/handle/10362/145647" target="_blank" rel="noopener noreferrer"><img src="images/RUN.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3390/app12168343" target="_blank" rel="noopener noreferrer">10.3390/app12168343</a>
				</td></tr>
				<tr><td>
					<i>TSSEARCH: Time Series Subsequence Search Library.</i> Folgado, D. Barandas, M., Antunes, M., Nunes, M. L., Liu, H., Hartmann, Y., Schultz, T., and Gamboa, H. (2022). SoftwareX, 18:101049. Elsevier.<br>
					<a href="https://scholar.google.com/scholar?q=TSSEARCH%3A+Time+Series+Subsequence+Search+Library" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/TSSEARCH%3A-Time-Series-Subsequence-Search-Library-Folgado-Barandas/88ae58d7183b13c72e67f90ee71e20cc87aa61f6" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/359648631_TSSEARCH_Time_Series_Subsequence_Search_Library" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S2352711022000425" target="_blank" rel="noopener noreferrer"><img src="images/Elsevier.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.aicos.fraunhofer.pt/en/publications.html" target="_blank" rel="noopener noreferrer"><img src="images/Aicos.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://novaresearch.unl.pt/en/publications/tssearch-time-series-subsequence-search-library" target="_blank" rel="noopener noreferrer"><img src="images/Nova.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://run.unl.pt/handle/10362/143322" target="_blank" rel="noopener noreferrer"><img src="images/RUN.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/fraunhoferportugal/tssearch" target="_blank" rel="noopener noreferrer"><img src="images/GitHub.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://tssearch.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer"><img src="images/TSSEARCH.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1016/j.softx.2022.101049" target="_blank" rel="noopener noreferrer">10.1016/j.softx.2022.101049</a>
				</td></tr>
				<tr><td>
					<i>CSL-SHARE: A Multimodal Wearable Sensor-Based Human Activity Dataset.</i> Liu, H.*, Hartmann, Y., and Schultz, T. (2021). Frontiers in Computer Science, 3:759136.<br>
					<a href="https://scholar.google.com/scholar?q=CSL-SHARE%3A+A+Multimodal+Wearable+Sensor-Based+Human+Activity+Datasets" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/CSL-SHARE%3A-A-Multimodal-Wearable-Sensor-Based-Human-Liu-Hartmann/17dbf3ac9e44c44f45589feffbf447afc0349068" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/355231971_CSL-SHARE_A_Multimodal_Wearable_Sensor-Based_Human_Activity_Dataset" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.frontiersin.org/articles/10.3389/fcomp.2021.759136" target="_blank" rel="noopener noreferrer"><img src="images/Frontiers.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.uni-bremen.de/en/csl/research/human-activity-recognition" target="_blank" rel="noopener noreferrer"><img src="images/Dataset.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.3389/fcomp.2021.759136" target="_blank" rel="noopener noreferrer">10.3389/fcomp.2021.759136</a>
				</td></tr>
				<tr><td>
					<i>TSFEL: Time Series Feature Extraction Library.</i> Barandas, M., Folgado, D., Fernandes, L., Santos, S., Abreu, M., Bota, P., Liu, H., Schultz, T., and Gamboa, H. (2020). SoftwareX, 11:100456. Elsevier.<br>
					<a href="https://scholar.google.com/scholar?q=TSFEL%3A+Time+series+feature+extraction+library+barandas" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a></b>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/TSFEL%3A-Time-Series-Feature-Extraction-Library-Barandas-Folgado/30d86ed68c163455b8f3ef10a5bf4c92ee8707bc" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/340085788_TSFEL_Time_Series_Feature_Extraction_Library" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S2352711020300017" target="_blank" rel="noopener noreferrer"><img src="images/Elsevier.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://novaresearch.unl.pt/en/publications/tsfel-time-series-feature-extraction-library" target="_blank" rel="noopener noreferrer"><img src="images/Nova.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://run.unl.pt/handle/10362/117283" target="_blank" rel="noopener noreferrer"><img src="images/RUN.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/fraunhoferportugal/tsfel" target="_blank" rel="noopener noreferrer"><img src="images/GitHub.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://tsfel.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer"><img src="images/TSFEL.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1016/j.softx.2020.100456" target="_blank" rel="noopener noreferrer">10.1016/j.softx.2020.100456</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<b>(Top-cited on <a href="https://www.journals.elsevier.com/softwarex" target="_blank" rel="noopener noreferrer">Software X</a>)
				</td></tr>
			
				<tr><td>
				</td></tr>
			
				<tr><td>
					<img src="images/Conference.png" height="15">&nbsp;&nbsp;
					<img src="images/2.png" height="15"><img src="images/3.png" height="15">&nbsp;&nbsp;|&nbsp;
					<img src="images/Highly.png" height="15" class = "pop1"> Best Paper Award (including "Student-Authored") <img src="images/2.png" height="15">&nbsp;&nbsp;|&nbsp;
					<img src="images/Candidate.png" height="15" class = "pop1"> Best Paper Finalists <img src="images/2.png" height="15">
				</td></tr>
			
				<tr><td>
					<img src="images/Forthcoming.png" height="20" class = "pop1">
					<i>A Review of Wearable Detectors for Toxic Environments: Devices, Mechanisms, Integration, and Application Scenarios.</i> Peter Mweetwa, Ezzegraoui Chorouk, Hui Liu*, Yanfeng Wu, Xiaofeng Zhao4, Yiqian Xu4, Xuhao Yan4 <br>
					<!--
					<a href="https://scholar.google.com/scholar?q=" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/" target="_blank" rel="noopener noreferrer"></a>
					--!>
				</td></tr>
			
				<tr><td>
					<img src="images/Forthcoming.png" height="20" class = "pop1">
					<i>.</i> <br>
					<!--
					<a href="https://scholar.google.com/scholar?q=" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/" target="_blank" rel="noopener noreferrer"></a>
					--!>
				</td></tr>
			
				<tr><td>
					<img src="images/Forthcoming.png" height="20" class = "pop1">
					<i>.</i> <br>
					<!--
					<a href="https://scholar.google.com/scholar?q=" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/" target="_blank" rel="noopener noreferrer"></a>
					--!>
				</td></tr>
			
				<tr><td>
					<img src="images/Forthcoming.png" height="20" class = "pop1">
					<i>.</i> <br>
					<!--
					<a href="https://scholar.google.com/scholar?q=" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/" target="_blank" rel="noopener noreferrer"></a>
					--!>
				</td></tr>
			
				<tr><td>
					<img src="images/Forthcoming.png" height="20" class = "pop1">
					<i>.</i> <br>
					<!--
					<a href="https://scholar.google.com/scholar?q=" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/" target="_blank" rel="noopener noreferrer"></a>
					--!>
				</td></tr>
			
				<tr><td>
					<img src="images/Forthcoming.png" height="20" class = "pop1">
					<i>.</i> <br>
					<!--
					<a href="https://scholar.google.com/scholar?q=" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/" target="_blank" rel="noopener noreferrer"></a>
					--!>
				</td></tr>
			
				<tr><td>
					<img src="images/Forthcoming.png" height="20" class = "pop1">
					<i>.</i> <br>
					<!--
					<a href="https://scholar.google.com/scholar?q=" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/" target="_blank" rel="noopener noreferrer"></a>
					--!>
				</td></tr>
			
				<tr><td>
					<i>The Bigger the Better? Towards EMG-Based Single-Trial Action Unit Recognition of Subtle Expressions. </i> Küster, D., Rammohan, R. A., Liu, H., Schultz, T., Koschke, R. (2025). In BIOSTEC 2025 - Proceedings of the 18th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 1: BIODEVICES, pages 100-110. INSTICC, SciTePress.<br>
					<img src="images/Highly.png" height="15" class = "pop1"> &nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://scholar.google.com/scholar?q=The+Bigger+the+Better%3F+Towards+EMG-Based+Single-Trial+Action+Unit+Recognition+of+Subtle+Expressions." target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/The-Bigger-the-Better-Towards-EMG-Based-Action-Unit-K%C3%BCster-Rammohan/4300ad9ff748225e3c69c01a1d16f6570a4afe0f" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/389374367_The_Bigger_the_Better_Towards_EMG-Based_Single-Trial_Action_Unit_Recognition_of_Subtle_Expressions" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0013389300003911" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0013389300003911" target="_blank" rel="noopener noreferrer">10.5220/0013389300003911</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<b>(<a href="https://biostec.scitevents.org/PreviousAwards.aspx#2025" target="_blank" rel="noopener noreferrer">Best Paper Award</a>)</b>
				</td></tr>
				<tr><td>
					<i>A Wearable Real-Time Human Activity Recognition System using Biosensors Integrated into a Knee Bandage.</i> Liu, H., and Schultz, T. (2019). In BIOSTEC 2019 - Proceedings of the 12th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 1: BIODEVICES, pages 47–55. INSTICC, SciTePress.<br>
					<img src="images/Highly.png" height="15" class = "pop1"> &nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://scholar.google.com/scholar?q=A+Wearable+Real-time+Human+Activity+Recognition+System+using+Biosensors+Integrated+into+a+Knee+Bandage." target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/A-Wearable-Real-time-Human-Activity-Recognition-a-Liu-Schultz/9d550549149560b091f92e0bcc294d063ee8856a" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/331779900_A_Wearable_Real-time_Human_Activity_Recognition_System_using_Biosensors_Integrated_into_a_Knee_Bandage" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://doi.org/10.5220/0007398800470055" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0007398800470055" target="_blank" rel="noopener noreferrer">10.5220/0007398800470055</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<b>(<a href="https://biostec.scitevents.org/PreviousAwards.aspx#2019" target="_blank" rel="noopener noreferrer">Best Paper Award</a>, Student Author)</b>
				</td></tr>
				<tr><td>
					<i>Merged Pitch Histogram and Pitch-Duration Histogram.</i> Liu, H., Xue, T., and Schultz, T. (2022). In SIGMAP 2022 - Proceedings of the 19th International Conference on Signal Processing and Multimedia Applications, pages 32-39. INSTICC, SciTePress.<br>
					<img src="images/Candidate.png" height="15" class = "pop1"> &nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://scholar.google.com/scholar?q=Merged+Pitch+Histograms+and+Pitch-Duration+Histograms" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Merged-Pitch-Histograms-and-Pitch-duration-Liu-Xue/1e4d841d8e492c2bad6a28b9f7275f5c1f64dd12" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/362055213_Merged_Pitch_Histograms_and_Pitch-duration_Histograms" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0007398800470055" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0011310300003289" target="_blank" rel="noopener noreferrer">10.5220/0011310300003289</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<b>(Best Paper Award Finalist)</b>
				</td></tr>
				<tr><td>
					<i>Interpretable High-Level Features for Human Activity Recognition.</i> Hartmann, Y., Liu, H., Lahrberg, S., and Schultz, T. (2022). In BIOSTEC 2022 - Proceedings of the 15th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 4: BIOSIGNALS, pages 40-49. INSTICC, SciTePress.<br>
					<img src="images/Candidate.png" height="15" class = "pop1"> &nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://scholar.google.com/scholar?q=Interpretable+high-level+features+for+human+activity+recognition" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Interpretable-High-level-Features-for-Human-Hartmann-Liu/df0bab3545d7509081bf2e5b40bca2e5333e25e9" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/358568253_Interpretable_High-Level_Features_for_Human_Activity_Recognition" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0010840500003123" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0010840500003123" target="_blank" rel="noopener noreferrer">10.5220/0010840500003123</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<b>(Best Student Paper Award Finalist)</b>
				</td></tr>
				<tr><td>
					<i>Longitudinal Data Acquisition for AI Services in Long-Term Care Facilities for Older Adults. </i> Paul, R. E., Kock, P., Hartmann, Y., Ball, E., Seibert, K., Liu, H., and Schultz, T. (2025). In BIOSTEC 2025 - Proceedings of the 18th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 2: HEALTHINF, pages 1099-1110. INSTICC, SciTePress.<br>
					<a href="https://scholar.google.com/scholar?q=Longitudinal+Data+Acquisition+for+AI+Services+in+Long-Term+Care+Facilities+for+Older+Adults" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Longitudinal-Data-Acquisition-for-AI-Services-in-Paul-Kock/dd4c3d35738cdbd631c63d033f3bfdb71ea386f1" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/390857914_Longitudinal_Data_Acquisition_for_AI_Services_in_Long-Term_Care_Facilities_for_Older_Adults" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0013403900003911" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0013403900003911" target="_blank" rel="noopener noreferrer">10.5220/0013403900003911</a>
				</td></tr>
				<tr><td>
					<i>LSTM-MorA: Melody-Accompaniment Classification of MIDI Tracks.</i> Liu, H.*, Flaack, L., Zhang, S., and Schultz, T. (2024). In ICANN 2024 - 33rd International Conference on Artificial Neural Networks, pages 443-485. Springer, Cham.<br>
					<a href="https://scholar.google.com/scholar?q=LSTM-MorA%3A+Melody-Accompaniment+Classification+of+MIDI+Tracks" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/LSTM-MorA%3A-Melody-Accompaniment-Classification-of-Liu-Flaack/43f200de156627f525ec8da377925d4afbbb3ed0" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/384068802_LSTM-MorA_Melody-Accompaniment_Classification_of_MIDI_Tracks" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.springerprofessional.de/lstm-mora-melody-accompaniment-classification-of-midi-tracks/50008576" target="_blank" rel="noopener noreferrer"><img src="images/Springer_Professional.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/chapter/10.1007/978-3-031-72356-8_29" target="_blank" rel="noopener noreferrer"><img src="images/SpringerLink.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1007/978-3-031-72356-8_29" target="_blank" rel="noopener noreferrer">10.1007/978-3-031-72356-8_29</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/book/10.1007/978-3-031-72356-8" target="_blank" rel="noopener noreferrer"><img src="images/Book.png" height="15"></a>
				</td></tr>
				<tr><td>
					<i>Examining the Effects of Human-Likeness of Avatars on Emotion Perception and Emotion Elicitation.</i> Zhang, S., Faruk, O., Porzel, R., Küster, D., Schultz, T., and Liu, H. (2024). In ABC 2024 - 6th International Conference on Activity and Behavior Computing, pages 1-12. IEEE.<br>
					<a href="https://scholar.google.com/scholar?q=Examining+the+Effects+of+Human-Likeness+of+Avatars+on+Emotion+Perception+and+Emotion+Elicitation" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Examining-the-Effects-of-Human-Likeness-of-Avatars-Zhang-Faruk/c907163319da8691caa6b410b00d06b8299fb138" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/383713116_Examining_the_Effects_of_Human-Likeness_of_Avatars_on_Emotion_Perception_and_Emotion_Elicitation" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/10652090/" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/ABC61795.2024.10652090" target="_blank" rel="noopener noreferrer">10.1109/ABC61795.2024.10652090</a>	</td></tr>
				<tr><td>
					<i>Comfort Assessment Method of EEG-Based Exoskeleton Walking-Assistive Device.</i> Zhou, C., Wang, H., Li, K., Liu, H., and Ye, X. (2024).  In BIOSTEC 2024 - Proceedings of the 17th International Joint Conference on Biomedical Engineering Systems and Technologies - BIOSIGNALS, pages 675-682. INSTICC, SciTePress.<br>
					<a href="https://scholar.google.com/scholar?q=Comfort+assessment+method+of+eeg-based+exoskeleton+walkingassistive+device" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Comfort-Assessment-Method-of-EEG-Based-Exoskeleton-Wang-Li/8b5a9f82a3210d70ce28ba26dbc0a715a729e14e" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/378485149_Comfort_Assessment_Method_of_EEG-Based_Exoskeleton_Walking-Assistive_Device" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0012564800003657" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0012564800003657" target="_blank" rel="noopener noreferrer">10.5220/0012564800003657</a>
				</td></tr>
				<tr><td>
					<i>Can Electromyography Alone Reveal Facial Action Units? A Pilot EMG-Based Action Unit Recognition Study with Real-Time Validation.</i> Veldanda, A., Liu, H., Koschke, R., Schultz, T., and Küster, D. (2024).  In BIOSTEC 2024 - Proceedings of the 17th International Joint Conference on Biomedical Engineering Systems and Technologies - BIODEVICES, pages 142–151. INSTICC, SciTePress.<br>
					<a href="https://scholar.google.com/scholar?q=Can+electromyography+alone+reveal+facial+action+units%3F+a+pilot+emg-based+action+unit+recognition+study+with+real-time+validation" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Can-Electromyography-Alone-Reveal-Facial-Action-A-Veldanda-Liu/a287e8b8b8b5967ad102cae91278b6d211f0e446" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/378482470_Can_Electromyography_Alone_Reveal_Facial_Action_Units_A_Pilot_EMG-Based_Action_Unit_Recognition_Study_with_Real-Time_Validation" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0012399100003657" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0012399100003657" target="_blank" rel="noopener noreferrer">10.5220/0012399100003657</a>
				</td></tr>
				<tr><td>
					<i>Really Can't Hold on Anymore? Physiological Indicators Versus Self-Reported Motivation Drop During Jogging.</i> Zhang, S., Kolensnikov, S., Rennspieß, T., Porzel, R., Schultz, T., and Liu, H. (2024).  In BIOSTEC 2024 - Proceedings of the 17th International Joint Conference on Biomedical Engineering Systems and Technologies - BIOSIGNALS, pages 821-831. INSTICC, SciTePress.<br>
					<a href="https://scholar.google.com/scholar?q=Really+Can’t+Hold+On+Anymore%3F+Physiological+Indicators+Versus+Self-Reported+Motivation+Drop+During+Jogging" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Really-Can't-Hold-On-Anymore-Physiological-Versus-Zhang-Kolensnikov/8e17b1337ebdea82e39e9cbb4c219cf5405741bc" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/378485922_Really_Can't_Hold_On_Anymore_Physiological_Indicators_Versus_Self-Reported_Motivation_Drop_During_Jogging" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0012577300003657" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0012577300003657" target="_blank" rel="noopener noreferrer">10.5220/0012577300003657</a>
				</td></tr>
				<tr><td>
					<i>Associating Endpoint Accuracy and Similarity of Muscle Synergies.</i> Cai, L., Yan, S., Ouyang, C., Zhang, T., Zhu, J., Chen, L., and Liu, H. (2024).  In BIOSTEC 2024 - Proceedings of the 17th International Joint Conference on Biomedical Engineering Systems and Technologies - BIOSIGNALS, pages 683-694. INSTICC, SciTePress.<br>
					<a href="https://scholar.google.com/scholar?q=Associating+endpoint+accuracy+and+similarity+of+muscle+synergies" target=" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Associating-Endpoint-Accuracy-and-Similarity-of-Cai-Yan/d804bfcbe0ef878941783c72f1d7113b00e427f8"_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/378484411_Associating_Endpoint_Accuracy_and_Similarity_of_Muscle_Synergies" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0012586800003657" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0012586800003657" target="_blank" rel="noopener noreferrer">10.5220/0012586800003657</a>
				</td></tr>
				<tr><td>
					<i>Integrated Driver Pose Estimation for Autonomous Driving.</i> Cao, X., Hu, W., and Liu, H. (2024).  In BIOSTEC 2024 - Proceedings of the 17th International Joint Conference on Biomedical Engineering Systems and Technologies - BIOSIGNALS, pages 695-702. INSTICC, SciTePress.<br>
					<a href="https://scholar.google.com/scholar?q=Integrated+Driver+Pose+Estimation+for+Autonomous+Driving" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Integrated-Driver-Pose-Estimation-for-Autonomous-Cao-Hu/e6816adff6e19ed4e9e6d8f226bcf0b0713fe2bf" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/378485657_Integrated_Driver_Pose_Estimation_for_Autonomous_Driving" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0012639400003657" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0012639400003657" target="_blank" rel="noopener noreferrer">10.5220/0012639400003657</a>
				</td></tr>
				<tr><td>
					<i>On a Real Real-Time Wearable Human Activity Recognition System.</i> Liu, H., Xue, T., and Schultz, T. (2023). In BIOSTEC 2023 - Proceedings of the 16th International Joint Conference on Biomedical Engineering Systems and Technologies - WHC, pages 711-720. INSTICC, SciTePress.<br> 
					<a href="https://scholar.google.com/scholar?q=on+a+real+real-time+wearable+human+activity+recognition+system" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/On-a-Real-Real-Time-Wearable-Human-Activity-System-Liu-Xue/1c0648a61e011468faf3289fcee39ec66b2a4d43" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/369015090_On_a_Real_Real-Time_Wearable_Human_Activity_Recognition_System" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0011927700003414" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0011927700003414" target="_blank" rel="noopener noreferrer">10.5220/0011927700003414</a>
				</td></tr>
				<tr><td>
					<i>Interactive and Interpretable Online Human Activity Recognition.</i> Hartmann, Y., Liu, H., and Schultz, T. (2022). In PERCOM 2022 - 20th IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events, pages 109–111. IEEE.<br>
					<a href="https://scholar.google.com/scholar?q=Interactive+and+Interpretable+Online+Human+Activity+Recognition" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Interactive-and-Interpretable-Online-Human-Activity-Hartmann-Liu/06f9c40b080318751f1667e9ac569d2da4abfd44" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/359692528_Interactive_and_Interpretable_Online_Human_Activity_Recognition" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/9767207" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/PerComWorkshops53856.2022.9767207" target="_blank" rel="noopener noreferrer">10.1109/PerComWorkshops53856.2022.9767207</a>
				</td></tr>
				<tr><td>
					<i>A Practical Wearable Sensor-Based Human Activity Recognition Research Pipeline.</i> Liu, H., Hartmann, Y., and Schultz, T. (2022). In BIOSTEC 2022 - Proceedings of the 15th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 5: HEALTHINF, pages 851-860. INSTICC, SciTePress.<br>
					<a href="https://scholar.google.com/scholar?q=A+Practical+Wearable+Sensor-Based+Human+Activity+Recognition+Research+Pipeline" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/A-Practical-Wearable-Sensor-based-Human-Activity-Liu-Hartmann/2e8a7e003683f7f6dca0ba3995c4c971cdfd9de9" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/358567885_A_Practical_Wearable_Sensor-Based_Human_Activity_Recognition_Research_Pipeline" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0010937000003123" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0010937000003123" target="_blank" rel="noopener noreferrer">10.5220/0010937000003123</a>
				</td></tr>
				<tr><td>
					<i>How Long Are Various Types of Daily Activities? Statistical Analysis of a Multimodal Wearable Sensor-Based Human Activity Dataset.</i> Liu, H., and Schultz, T. (2022). In BIOSTEC 2022 - Proceedings of the 15th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 5: HEALTHINF, pages 684-692. INSTICC, SciTePress.<br>
					<a href="https://scholar.google.com/scholar?q=How+long+are+various+types+of+daily+activities%3F+statistical+analysis+of+a+multimodal+wearable+sensor-based+human+activity+dataset" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/How-Long-Are-Various-Types-of-Daily-Activities-of-a-Liu-Schultz/b1be768408ec57e85573824e9acee3b6db7376ac" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/358568068_How_Long_Are_Various_Types_of_Daily_Activities_Statistical_Analysis_of_a_Multimodal_Wearable_Sensor-Based_Human_Activity_Dataset" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0010896400003123" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0010896400003123" target="_blank" rel="noopener noreferrer">10.5220/0010896400003123</a>
				</td></tr>
				<tr><td>
					<i>Hidden Markov Model and Its Application in Human Activity Recognition and Fall Detection: A Review.</i> Xue, T., and Liu, H. (2022). Communications, Signal Processing, and Systems. Springer, Singapore.<br>
					<a href="https://scholar.google.com/scholar?q=Hidden+Markov+Model+and+Its+Application+in+Human+Activity+Recognition+and+Fall+Detection%3A+A+Review" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Hidden-Markov-Model-and-Its-Application-in-Human-A-Xue-Liu/b37df2bc1532b776845dd48e4fa034c47a3dc39c" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/355198894_Hidden_Markov_Model_and_Its_Application_in_Human_Activity_Recognition_and_Fall_Detection_A_Review" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.springerprofessional.de/hidden-markov-model-and-its-application-in-human-activity-recogn/20265550" target="_blank" rel="noopener noreferrer"><img src="images/Springer_Professional.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/chapter/10.1007/978-981-19-0390-8_108" target="_blank" rel="noopener noreferrer"><img src="images/SpringerLink.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1007/978-981-19-0390-8_108" target="_blank" rel="noopener noreferrer">10.1007/978-981-19-0390-8_108</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/book/10.1007/978-981-19-0390-8" target="_blank" rel="noopener noreferrer"><img src="images/Book.png" height="15"></a>
				</td></tr>
				<tr><td>
					<i>Motion Units: Generalized Sequence Modeling of Human Activities for Sensor-Based Activity Recognition.</i> Liu, H., Hartmann, Y., and Schultz, T. (2021). In EUSIPCO 2021 - 29th European Signal Processing Conference. IEEE.<br>
					<a href="https://scholar.google.com/scholar?q=Motion+Units%3A+Generalized+Sequence+Modeling+of+Human+Activities+for+Sensor-Based+Activity+Recognition" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Motion-Units%3A-Generalized-Sequence-Modeling-of-for-Liu-Hartmann/7299d035bdb6e37058dd28f123e190fd4915a5e4" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/356883313_Motion_Units_Generalized_Sequence_Modeling_of_Human_Activities_for_Sensor-Based_Activity_Recognition" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/document/9616298" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.23919/EUSIPCO54536.2021.9616298" target="_blank" rel="noopener noreferrer">10.23919/EUSIPCO54536.2021.9616298</a>
				</td></tr>
				<tr><td>
					<i>Feature Space Reduction for Human Activity Recognition Based on Multi-Channel Biosignals.</i> Hartmann, Y., Liu, H., and Schultz, T. (2021). In BIOSTEC 2021 - Proceedings of the 14th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 4: BIOSIGNALS, pages 215-222. INSTICC, SciTePress.<br>
					<a href="https://scholar.google.com/scholar?q=Feature+Space+Reduction+for+Human+Activity+Recognition+based+on+Multi-channel+Biosignals." target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Feature-Space-Reduction-for-Human-Activity-based-on-Hartmann-Liu/98ebf6536aaaef7689f850664903c8c877843cd7" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/349384072_Feature_Space_Reduction_for_Human_Activity_Recognition_based_on_Multi-channel_Biosignals" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0010260802150222" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0010260802150222" target="_blank" rel="noopener noreferrer">10.5220/0010260802150222</a>
				</td></tr>
				<tr><td>
					<i>Feature Space Reduction for Multimodal Human Activity Recognition.</i> Hartmann, Y., Liu, H., and Schultz, T. (2020). In BIOSTEC 2020 - Proceedings of the 13th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 4: BIOSIGNALS, pages 135–140. INSTICC, SciTePress.<br>
					<a href="https://scholar.google.com/scholar?q=Feature+Space+Reduction+for+Multimodal+Human+Activity+Recognition." target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Feature-Space-Reduction-for-Multimodal-Human-Hartmann-Liu/34bce7b4cbd0600001597c1d5c683a4090df7187" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/a339903026_Feature_Space_Reduction_for_Multimodal_Human_Activity_Recognition" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0008851401350140" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0008851401350140" target="_blank" rel="noopener noreferrer">10.5220/0008851401350140</a>
				</td></tr>
				<tr><td>
					<i>ASK: A Framework for Data Acquisition and Activity Recognition.</i> Liu, H., and Schultz, T. (2018). In BIOSTEC 2018 - Proceedings of the 11th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 3: BIOSIGNALS, pages 262–268. INSTICC, SciTePress.
					<br>
					<a href="https://scholar.google.com/scholar?q=ASK%3A+A+Framework+for+Data+Acquisition+and+Activity+Recognition." target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/ASK%3A-A-Framework-for-Data-Acquisition-and-Activity-Liu-Schultz/0321ad383bc44905ba73967a87570363cc57cd12" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/322873353_ASK_A_Framework_for_Data_Acquisition_and_Activity_Recognition" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0006732902620268" target="_blank" rel="noopener noreferrer"><img src="images/SciTePress.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.5220/0006732902620268" target="_blank" rel="noopener noreferrer">10.5220/0006732902620268</a>
				</td></tr>
				<tr><td>
					<i>Bremen Big Data Challenge 2017: Predicting University Cafeteria Load.</i> Weiner, J., Diener, L., Stelter, S., Externest, E., Kühl, S., Herff, C., Putze, F., Schulze, T., Salous, M., Liu, H., Küster, D., and Schultz, T. (2017). KI 2017: Advances in Artificial Intelligence. Springer, Cham.<br>
					<a href="https://scholar.google.com/scholar?q=Bremen+Big+Data+Challenge+2017%3A+Predicting+University+Cafeteria+Load" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Bremen-Big-Data-Challenge-2017%3A-Predicting-Load-Weiner-Diener/8ba8727f1e80e598619788888ace58effcaa51de" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/319890452_Bremen_Big_Data_Challenge_2017_Predicting_University_Cafeteria_Load" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.springerprofessional.de/bremen-big-data-challenge-2017-predicting-university-cafeteria-l/15121170" target="_blank" rel="noopener noreferrer"><img src="images/Springer_Professional.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/chapter/10.1007/978-3-319-67190-1_35" target="_blank" rel="noopener noreferrer"><img src="images/SpringerLink.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1007/978-3-319-67190-1_35" target="_blank" rel="noopener noreferrer">10.1007/978-3-319-67190-1_35</a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://link.springer.com/book/10.1007/978-3-319-67190-1" target="_blank" rel="noopener noreferrer"><img src="images/Book.png" height="15"></a>
				</td></tr>
				<tr><td>
					<i>Capacity of Cooperative Ad Hoc Networks with Heterogeneous Traffic Patterns.</i> Liu, H., and Wang, X. (2011). In ICC 2011 - IEEE International Conference on Communications, pages 1–5. IEEE.<br>
					<a href="https://scholar.google.com/scholar?q=Capacity+of+Cooperative+Ad+Hoc+Networks+with+Heterogeneous+Traffic+Patterns" target="_blank" rel="noopener noreferrer"><img src="images/GoogleScholar.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.semanticscholar.org/paper/Capacity-of-Cooperative-Ad-Hoc-Networks-with-Liu-Wang/b4abbc50f034d33c326ee11f98d1d7e9e189d8e2" target="_blank" rel="noopener noreferrer"><img src="images/Semantic.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.researchgate.net/publication/224249853_Capacity_of_Cooperative_Ad_Hoc_Networks_with_Heterogeneous_Traffic_Patterns" target="_blank" rel="noopener noreferrer"><img src="images/ResearchGate.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://ieeexplore.ieee.org/abstract/document/5963075" target="_blank" rel="noopener noreferrer"><img src="images/IEEE.png" height="15"></a>
					&nbsp;&nbsp;&nbsp;&nbsp;<img src="images/doi.png" height="15"><a href="https://doi.org/10.1109/icc.2011.5963075" target="_blank" rel="noopener noreferrer">10.1109/icc.2011.5963075</a>
				</td></tr>
			</table></td></tr>

			<tr><td></td></tr><tr><td></td></tr>

			<tr><td><h1 align="left"><span style="text-decoration: overline">学界贡献</span></h1></td><tr>
			<tr><td><table align="left" cellpadding="15">
				<tr>
					<td align="center">编委</td>
					<td>
						&#9679; <img src="images/Lippincott.png" height="15" class="slightpop"> <a href="https://journals.lww.com/rmr/pages/editorialboard.aspx" target="_blank" rel="noopener noreferrer">Regenerative Medicine Reports</a><br>
						&#9679; <img src="images/IOP.png" height="15" class="slightpop"> <a href="https://publishingsupport.iopscience.iop.org/journals/engineering-research-express/editorial-board/" target="_blank" rel="noopener noreferrer">Engineering Research Express</a><br>
						&#9679; <img src="images/Frontiers.png" height="15" class="slightpop"> in <a href="https://loop.frontiersin.org/people/1257413/overview" target="_blank" rel="noopener noreferrer">Big Data</a><br>
						&#9679; <img src="images/Frontiers.png" height="15" class="slightpop"> in <a href="https://loop.frontiersin.org/people/1257413/overview" target="_blank" rel="noopener noreferrer">Artificial Intelligence</a>
					</td>
				</tr>
				<tr>
					<td align="center">编辑</td>
					<td>
						&#9679; <img src="images/Frontiers.png" height="15" class="slightpop"> in Behavioral Neuroscience<br>
						&#9679; <img src="images/Frontiers.png" height="15" class="slightpop"> in Immunology<br>
						&#9679; <img src="images/Frontiers.png" height="15" class="slightpop"> in Bioengineering and Biotechnology<br>
						&#9679; <img src="images/Frontiers.png" height="15" class="slightpop"> in Endocrinology
					</td>
				</tr>
				<tr>
					<td align="center">特邀编委</td>
					<td>
						&#9679; <img src="images/MDPI.png" height="15" class="slightpop"> <img src="images/Biosensors.png" height="15"> <a href="https://www.mdpi.com/journal/biosensors/special_issues/70678AF74D" target="_blank" rel="noopener noreferrer"><i>Innovative Technologies of Acquiring, Processing, Modeling, or Utilizing Biomedical and Physiological Signals from Biosensing</i></a><br>
						&#9679; <img src="images/MDPI.png" height="15" class="slightpop"> <img src="images/Sensors.png" height="15"> <a href="https://www.mdpi.com/journal/sensors/special_issues/L9E9BHCLP8" target="_blank" rel="noopener noreferrer"><i>Electroencephalogram/Electromyogram-Based Sensing Technologies for Biomedical Applications: Challenges and Possible Applications</i></a><br>
						&#9679; <img src="images/MDPI.png" height="15" class="slightpop"> <img src="images/Sensors.png" height="15"> <a href="https://www.mdpi.com/journal/sensors/special_issues/Sensors_Human_Activity_Recognition" target="_blank" rel="noopener noreferrer"><i>Sensors for Human Activity Recognition: 1<sup>st</sup> Edition</i></a><br>
						&#9679; <img src="images/MDPI.png" height="15" class="slightpop"> <img src="images/Sensors.png" height="15"> <a href="https://www.mdpi.com/journal/sensors/special_issues/671TM05M9J" target="_blank" rel="noopener noreferrer"><i>Sensors for Human Activity Recognition: 2<sup>nd</sup> Edition</i></a><br>
						&#9679; <img src="images/MDPI.png" height="15" class="slightpop"> <img src="images/Biosensors.png" height="15"> <img src="images/Sensors.png" height="15"> <a href="https://www.mdpi.com/journal/biosensors/special_issues/F99X76258C" target="_blank" rel="noopener noreferrer"><i>Sensors for Human Activity Recognition: 3<sup>rd</sup> Edition</i></a>
					</td>
				</tr>
				<tr>
					<td align="center">学术会议联合主席（Co-Chair）</td>
					<td>
						&#9679; <a href="https://www.is-aii.org/committees/" target="_blank" rel="noopener noreferrer"><img src="images/IS-ALL2025.png" height="15"></a> 2025 – International Symposium on Artificial Intelligence Innovations, 中国贵阳<br>
						&#9679; <a href="https://www.icairsa.net/?page_id=9629" target="_blank" rel="noopener noreferrer"><img src="images/AIRSA2025.png" height="15"></a> 2025 – International Confer	ence on Artificial Intelligence and Remote Sensing Applications, 中国长沙
					</td>
				</tr>
				<tr>
					<td align="center">学术会议技术委员会主席 (TPC)</td>
					<td>
						&#9679; <a href="https://www.artinhci.com/Committee.aspx" target="_blank" rel="noopener noreferrer"><img src="images/ArtInHCI.png" height="15" class="pop5"></a> 2025 – 3<sup>rd</sup> International Conference on Artificial Intelligence and Human-Computer Interaction, 中国南宁
					</td>
				</tr>
				<tr>
					<td align="center">学术会议程序委员会联合主席（Co-PC）</td>
					<td>
						&#9679; <a href="https://biosignals.scitevents.org/?y=2024" target="_blank" rel="noopener noreferrer"><img src="images/BIOSIGNALS2024.png" height="12"></a> – 17<sup>th</sup> International Conference on Bio-Inspired Systems and Signal Processing (in <a href="https://biostec.scitevents.org/?y=2024" target="_blank" rel="noopener noreferrer"><img src="images/BIOSTEC2024.png" height="12"></a>), 意大利罗马<br>
						&#9679; <a href="https://biosignals.scitevents.org/?y=2026" target="_blank" rel="noopener noreferrer"><img src="images/BIOSIGNALS2026.png" height="12"></a> – 19<sup>th</sup> International Conference on Bio-Inspired Systems and Signal Processing (in <a href="https://biostec.scitevents.org/?y=2026" target="_blank" rel="noopener noreferrer"><img src="images/BIOSTEC2026.png" height="12"></a>), 西班牙马贝拉
					</td>
				</tr>
				<tr>
					<td align="center">学术会议技术委员会联合主席（Co-TPC）</td>
					<td>
						&#9679; <img src="images/ArtInHCI.png" height="15" class="pop5"> 2024 – 2<sup>nd</sup> International Conference on Artificial Intelligence and Human-Computer Interaction, 中国昆明
					</td>
				</tr>
				<tr>
					<td align="center">学术会议区域主席（AC）</td>
					<td>
						&#9679; <a href="https://e-nns.org/icann2024/organization/program-committee/" target="_blank" rel="noopener noreferrer"><img src="images/ICANN2024.png" height="15" class="pop5"></a> - 33<sup>rd</sup> International Conference on Artificial Neural Networks, 瑞士卢加诺<br>
						&#9679; <a href="https://ceii.asia/2024/committee.html" target="_blank" rel="noopener noreferrer"><img src="images/CEII2024.png" height="15" class="pop5"></a> – 7<sup>th</sup> Asia Conference on Cognitive Engineering and Intelligent Interaction, 新加坡
					</td>
				</tr>
				<tr>
					<td align="center">学术会议宣传主席（Publicity Chair）</td>
					<td>
						&#9679; <a href="https://autocare.ai/abc2025" target="_blank" rel="noopener noreferrer"><img src="images/ABC2025.png" height="15" class="pop5"></a> – 7<sup>th</sup>  International Conference on Activity and Behavior Computing, 阿联酋阿布扎比<br>
						&#9679; <a href="https://www.icaigc.org/committee.html" target="_blank" rel="noopener noreferrer"><img src="images/AIGC2024.png" height="15" class="pop5"></a> – 2<sup>nd</sup>  International Conference on AI-generated Content, 中国北京<br>
						&#9679; ICCI 2025（筹） – International Conference on AI-generated Content, 中国香港
					</td>
				</tr>
				<tr>
					<td align="center">博士生论坛联合主席（Doctoral Consortium Co-Chair）</td>
					<td>
						&#9679; <a href="https://biostec.scitevents.org/DoctoralConsortium.aspx/?y=2025" target="_blank" rel="noopener noreferrer"><img src="images/BIOSTEC2025.png" height="12"></a> – 18<sup>th</sup> International Joint Conference on Biomedical Engineering Systems and Technologies, 葡萄牙波尔图
					</td>
				</tr>
				<tr>
					<td align="center">学术会议分会场主席（Session Chair）</td>
					<td>
						&#9679; <a href="https://healthinf.scitevents.org/?y=2022" target="_blank" rel="noopener noreferrer"><img src="images/HEALTHINF22.png" height="12"></a> – 15<sup>th</sup> International Conference on Health Informatics, Vienna, Austria<br>
						&#9679; <a href="https://sigmap.scitevents.org/?y=2022" target="_blank" rel="noopener noreferrer"><img src="images/SIGMAP22.png" height="12"></a> – 19<sup>th</sup> International Conference on Signal Processing and Multimedia Applications, Lisbon, Portugal<br>
						&#9679; <a href="https://biostec0.scitevents.org//?y=2023" target="_blank" rel="noopener noreferrer"><img src="images/BIOSTEC2023.png" height="12"></a> – 16<sup>th</sup> International Joint Conference on Biomedical Engineering Systems and Technologies, Lisbon, Portugal<br>
						&#9679; <a href="https://healthinf.scitevents.org/?y=2025" target="_blank" rel="noopener noreferrer"><img src="images/HEALTHINF25.png" height="12"></a> – 18<sup>th</sup> International Conference on Health Informatics, Porto, Portugal
					</td>
				</tr>
				<tr>
					<td align="center"><a href="reviewer.html" target="_blank" rel="noopener noreferrer">审稿人</a></td>
					<td>
						&#9679; 43本国际期刊，包括Nat. Commun.、Inform. Fusion, IEEE Sens. J.、Biomed. Signal Process. Control、Geo-Spat. Inf. Sci., Remote Sens.、Front. Plant Sci.、Front Bioeng. Biotechnol.、Physiol. Meas.、Phys. Scr.、 ISPRS Int. J. Geo-Inf.、Biomimetics、Biosensors等<br>
						&#9679; 多个国际会议, 包括IEEE BSN, BHI; ACM ICMI, ISWC, AUTOMOTIVEUI; ITP<br>
						&#9679; <a href="images/IOP_Certificate.png" target="_blank" rel="noopener noreferrer">IOP Trusted Reviewer</a> 表彰“极其高水平的同行评审胜任力”（<i>an exceptionally high level of peer review competency</i>）<br>
						&#9679; Sensors <a href="https://www.mdpi.com/journal/sensors/awards/2188" target="_blank" rel="noopener noreferrer">2023 最佳审稿人奖</a>
					</td>
				</tr>
				<tr>
					<td align="center">国际学术会议主旨报告、演讲和艺术讲座</td>
					<td>
						&#9679; <a href="http://www.iccmid.org/speakers/" target="_blank" rel="noopener noreferrer"><img src="images/CMID.png" height="15" class="pop5"></a> – 第4届临床医学与智能设备国际研讨会，德国海德堡，2025：<i>穿戴式人工智能</i><br>
						&#9679; <img src="images/CAMT.png" height="15" class="pop5"> 同济大学中德工程学院，德国杜塞尔多夫，2018：<i>学术与艺术</i>  <img src="images/CAMTTalk.jpg" height="12" class="pop40"><br>
						&#9679; <a href="https://biostec.scitevents.org/?y=2019" target="_blank" rel="noopener noreferrer"><img src="images/BIOSTEC2019.png" height="12"></a> – 第12届国际生物医学工程系统和技术联合会议，捷克布拉格，2019：<a href="https://biostec.scitevents.org/Tutorials.aspx?y=2019#3" target="_blank" rel="noopener noreferrer"><i>From Offline towards Real-Time</i></a> (Tutorial)<br>
						&#9679; <a href="https://kd2school.info/" target="_blank" rel="noopener noreferrer"><img src="images/KD2School.png" height="12" class="pop10"></a> 卡尔斯鲁厄理工学院KD<sup>2</sup>school适应性系统年度座谈会，德国卡尔斯鲁厄，2022：<i>智能膝盖绷带</i> <img src="images/KD2School_Workshop.png" height="12" class="pop20">
				</tr>
				<tr>
				<td align="center">开源代码库、大数据和在线教程贡献者</td>
					<td>
						&#9679; Biosignals Notebooks <img src="images/biosignalsnotebooks.png" height="12" class="pop10">：传感器信号处理和人工智能线上教程和代码集<br>
						&#9679; <a href="https://tsfel.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer">TSFEL</a>
						<a href="https://tsfel.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer"><img src="images/TSFEL.png" height="12" class="pop10"></a>：时序信号特征提取代码库<br>
						&#9679; <a href="https://tssearch.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer">TSSEARCH</a>
						<a href="https://tssearch.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer"><img src="images/TSSEARCH.png" height="12" class="pop10"></a> ：时序信号子序列搜索代码库<br>
						&#9679; <a href="https://www.uni-bremen.de/en/csl/research/human-activity-recognition" target="_blank" rel="noopener noreferrer">CSL-SHARE</a>:
						多模态可穿戴式人体动作传感大数据集<br>
						&#9679; <a href="https://www.uni-bremen.de/en/csl/research/sensorder-artifact-classification-during-biosignal-acquisition" target="_blank" rel="noopener noreferrer">sensORder</a>:
						心电采集过程中的各种实时伪影数据集
					</td>
				</tr>
				<tr>
					<td align="center">
						大数据竞赛负责人
					</td>
					<td>
						&#9679; <img src="images/BBDC2019.png" height="12" class="pop25"> 第4届不来梅—北德大数据竞赛 BBDC 2019：（独立承担）大数据采集、处理和提供工作。北德数十所高校的60多支队伍参加最后的角逐</br>
						&#9679; <img src="images/BBDC2024.png" height="12" class="pop25"> 第9届 <a href="https://bbdc.csl.uni-bremen.de/en/2024-2/pupils/" target="_blank" rel="noopener noreferrer">BBDC 2024</a>：Basic Track总负责人。获得德国近十所高校以及来北德多个教育主管机构的广泛关注与支持
					</td>
				</tr>
			</table></td></tr>

			<tr><td></td></tr><tr><td></td></tr>


			<tr><td><h1 align="left"><span style="text-decoration: overline">项目基金</span></h1></td><tr>
			<tr><td><table align="left" cellpadding="15">
				<tr>
					<td align="center"><a href="https://www.uni-bremen.de/en/csl/projects/past-projects/arthrokinemat" target="_blank" rel="noopener noreferrer"><img src="images/Arthrokinemat.png" height="50" class="pop5"></a><br>Arthrokinemat (2016 — 2019)</td>
					<td>
						&#9679; 主研德国联邦经济事务与气侯行动部（BMWi）Arthrokinemat项目<br>
						&#9679; 康复工程中的集成传感器智能膝盖绷带<br>
						&#9679; 已经成功结题
					</td>
				</tr>
				<tr>
					<td align="center"><a href="https://www.uni-bremen.de/en/csl/projects/current-projects/nf-bwb-promoting-young-talents-supported-by-bremen-securities-exchange-foundation" target="_blank" rel="noopener noreferrer"><img src="images/BWB.png" height="60"></a><br>NF-BWB (2023 — 2025)</td>
					<td>
						&#9679; 主研德国不来梅证券基金会（BWB）资助的NF-BWB"青年英才培养"项目  <a href="images/BBDCB23.png" target="_blank" rel="noopener noreferrer"><img src="images/BBDCB23.png" height="12" class="pop40"></a><br>
						&#9679; 在多所高校普及人工智能和大数据，并举办竞赛
					</td>
				</tr>
				<tr>
					<td align="center"><a href="https://www.uni-bremen.de/en/csl/projects/current-projects/intel4coro" target="_blank" rel="noopener noreferrer"><img src="images/IntEL4CoRo.png" height="50" class="pop5"></a><br>IntEL4CoRo (2021 — 2025)</td>
					<td>
						&#9679; 主研德国联邦教育及研究部（BMBF）IntEL4CoRo项目<br>
						&#9679; 智能机器人设计仿真教学平台
					</td>
				</tr>
				<tr>
					<td align="center"><a href="https://www.uni-bremen.de/en/csl/projects/current-projects/etap" target="_blank" rel="noopener noreferrer"><img src="images/ETAP.png" height="60" class="pop5"></a><br>ETAP (2022 — 2025)</td>
					<td>
						&#9679; 参与起草德国联邦卫生部（BMG）ETAP项目<br>
						&#9679; 基于人工智能运动监测的长期护理中半自动化护理优化
					</td>
				</tr>
				<tr>
					<td align="center"><a href="https://yerun.eu/" target="_blank" rel="noopener noreferrer"><img src="images/YERUN.png" height="40"></a><br>欧洲青年研究型大学网络</td>
					<td>&#9679; YERUN学者：<a href="https://yerun.eu/2022/09/yerun-research-mobility-awards-2022-results-out-now-check-out-who-has-been-awarded/" target="_blank" rel="noopener noreferrer">科研流动奖金</a></td>
				</tr>
				<tr>
					<td align="center"><a href="https://www.unl.pt" target="_blank" rel="noopener noreferrer"><img src="images/Nova.png" height="40"></a><br>葡萄牙新里斯本大学</td>
					<td>&#9679; <a href="https://www.uni-bremen.de/lehre-studium/lehren-und-lernen-international/erasmus-dozentenmobilitaet" target="_blank" rel="noopener noreferrer"><img src="images/Erasmus.svg" height="12" class="pop15"></a> 欧盟伊拉斯谟海外教学奖学金</td>
				</tr>
			</table></td></tr>

			<tr><td></td></tr><tr><td></td></tr>

			<tr><td><h1 align="left"><span style="text-decoration: overline">产研活动</span></h1></td><tr>
			<tr><td><table align="left" cellpadding="15">
				<tr>
					<td align="center">
						<a href="https://www.pluxbiosignals.com/" target="_blank" rel="noopener noreferrer"><img src="images/PLUX.png" height="60"></a><br>
						2019年以来<br>
						PLUX无线生物信号传感器公司<br>
						葡萄牙里斯本
					</td>
					<td>
						&#9679; 特聘<a href="https://qrco.de/bdHmtW" target="_blank" rel="noopener noreferrer">高级研发总监</a><br>
						&#9679; 组织企业和大学之间的合作研究、教育和社会活动<br>
						&#9679; 研发实时识别与反馈技术<br>
						&#9679; 为多模态可穿戴生物医学信号采集软件的开发提供咨询<br>
						&#9679; 参与开源《Biosignal Notebooks》教程的策划并编写代码、撰写章节<br>
						&#9679; 规划、协助和开展在公司在德国和亚洲的业务扩展活动<br>
						&#9679; 派遣和指导博士生、硕士生和本科生进行跨境科研实习或论文答辩<br>
						&#9679; 发表企业产品的优秀研究成果<br>
					<td>
				</tr>
				<tr>
					<td align="center"><a href="https://bremen.ai/" target="_blank" rel="noopener noreferrer"><img src="images/BremenAI.png" height="70"></a><br>不来梅人工智能展览会<br>德国不来梅 2019</td>
					<td>
						&#9679; 技术演示：实时人体动作识别系统<br>
						&#9679; 互动演示：MR（混合现实）康复辅助游戏平台<br>
						&#9679; 举办地：<a href="https://www.mevis.fraunhofer.de/de.html" target="_blank" rel="noopener noreferrer"><img src="images/MEVIS.png" height="12" class="pop5"></a> 弗劳恩霍夫协会数字医学研究所
					</td>
				</tr>
				<tr>
					<td align="center"><img src="images/MindDay.png" height="40"><br>“智慧头脑日”展览研讨会<br>德国不来梅 2021</td>
					<td>
						&#9679; 技术演示：人工智能膝盖绷带 <img src="images/MindDay21.jpg" height="12" class="pop40"><br>
						&#9679; 举办地：<a href="https://www.uebersee-museum.de/" target="_blank" rel="noopener noreferrer"><img src="images/Museum.png" height="12"></a> 不来梅海外博物馆</td>
					</td>
				</tr>
				<tr>
					<td align="center"><a href="https://www.bridge-online.de/campusideen" target="_blank" rel="noopener noreferrer"><img src="images/CAMPUSiDEEN22.jpg" height="50" class="pop5"></a><br>CAMPUSiDEEN 2022<br>商业创意大赛<br></td>
					<td>&#9679; <a href="https://bridge-online.de/en/campusideen/review/2022-3rd-audience-award" target="_blank" rel="noopener noreferrer">铜奖</a> <img src="images/CAMPUSiDEEN.png" height="12" class="pop40"><td>
				</tr>
				<tr>
					<td align="center"><img src="images/Golden_Panda.png" height="50"><br>金熊猫全球创新创业大赛 2022<br>成都市人民政府举办</td>
					<td>&#9679; 欧洲区评委、宣传大使<td>
				</tr>
				<tr>
					<td align="center">
						<a href="" target="_blank" rel="noopener noreferrer"><img src="images/SAC.png" height="30" class="pop1"></a><br>
						2024年以来<br>
						中国华电集团有限公司<br>
						国电南京自动化股份有限公司
					</td>
					<td>
						&#9679; （特邀）首席专家
					</td>
				</tr>
			</table></td></tr>
			<tr><td></td></tr><tr><td></td></tr>

			<tr><td><h1 align="left"><span style="text-decoration: overline">专业履历</span></h1></td></tr>
			<tr><td><table align="left" cellpadding="15">
				<tr>
					<td align="center">
						<a href="http://www.shixi.edu.sh.cn/" target="_blank" rel="noopener noreferrer"><img src="images/Shixi.png" width="60"></a><br>
						2001 — 2004<br>
						上海市市西中学
					</td>
					<td>
						&#9679; 高中，期间获得：<br>
						&#9679; 全国青少年信息学奥林匹克联赛提高组一等奖<br>
						&nbsp;&nbsp;（证书编号：<a href="https://www.chsi.com.cn/mdgs2004/bss/info_sj/200504/20050401/5922.html" target="_blank" rel="noopener noreferrer">I030473</a>）<br>
						&#9679; 上海市CASIO杯程序设计竞赛高中组一等奖
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.sjtu.edu.cn/" target="_blank" rel="noopener noreferrer"><img src="images/SJTU.svg" width="60"></a><br>
						2004 — 2007<br>
						上海交通大学<br>
						电子信息与电气工程学院<br>
						通信工程专业
					</td>
					<td>
						<!--
						&nbsp;&nbsp; <img src="images/BachelorDegree.png" height="40" class="pop15">&nbsp;
						<img src="images/BachelorDiploma.png" height="40" class="pop15"><br>
						-->
						&#9679; 本科学位<br>
						&#9679; 上海交通大学奖学金（三年）
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.tu.berlin/" target="_blank" rel="noopener noreferrer"><img src="images/TUB.png" width="60"></a><br>
						2007 — 2009<br>
						柏林工业大学<br>
						电子学与信息学系（四系）<br>
						<a href="https://www.nue.tu-berlin.de/menue/home/" target="_blank" rel="noopener noreferrer">信号传输专业</a><br>
						<a href="https://www.tu.berlin/qu/" target="_blank" rel="noopener noreferrer">通信质量和适用性专业</a><br>
						<a href="https://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/" target="_blank" rel="noopener noreferrer">机器视觉专业</a>
					</td>
					<td>
						&nbsp;&nbsp;
						<!--
						<img src="images/Diplom.png" height="40" class="pop15">
						-->
						<img src="images/DiplomNoten.png" height="40" class="pop20"><br>
						&#9679; 第一硕士学位：Diplom-Ingenieur<br>
						&#9679; 课程平均分：1.0<br>
						&#9679; 担任研究助理，于<br>&nbsp;&nbsp;&nbsp;<a href="https://www.hhi.fraunhofer.de/index.html" target="_blank" rel="noopener noreferrer"><img src="images/HHI.png" height="12" class="pop5"></a> 弗劳恩霍夫协会赫兹通讯研究所
					</td>
				<tr>
				<tr>
					<td align="center">
						<a href="https://www.sjtu.edu.cn/" target="_blank" rel="noopener noreferrer"><img src="images/SJTU.svg" width="60"></a><br>
						2009 — 2011<br>
						上海交通大学<br>
						电子信息与电气工程学院<br>
						通信与信息系统专业
					</td>
					<td>
						&nbsp;&nbsp; <img src="images/MasterGPA.png" height="50" class="pop15">&nbsp;
						<img src="images/MasterRanking.png" height="50" class="pop15"><br>
						&#9679; 第二硕士学位：Master of Science<br>
						&#9679; 学积分/积点/GPA：3.16/3.3<br>
						&#9679; GPA排名：1/106<br>
						&#9679; 国家优秀研究生一等奖学金
					</td>
				</tr>
				<tr>
					<td align="center"><a href="https://www.tu.berlin/" target="_blank" rel="noopener noreferrer"><img src="images/TUB.png" width="60"></a><br>2012 — 2013<br>柏林工业大学</td>
					<td>&#9679; 科研助理<br>&#9679; 柏林工业大学校长奖学金</td>
				</tr>
				<tr>
					<td align="center"><img src="images/FP.png" width="60"><br>2014 — 2016<br>企业界工作</td>
					<td>&#9679; 软件工程师<br>&#9679; 负责集成开发项目</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.uni-bremen.de/" target="_blank" rel="noopener noreferrer"><img src="images/UB.png" height="40"></a>&nbsp;&nbsp;
						<a href="https://www.uni-bremen.de/csl" target="_blank" rel="noopener noreferrer"><img src="images/CSL.svg" height="40"></a><br>
						2016 — 2021<br>
						不来梅大学<br>
						数学和信息学系（三系）<br>
						认知系统实验室（CSL）
					</td>
					<td>
						&#9679; 科研项目和教学工作，2021年获工程学博士学位 <a href="videos/2021_Verteidigung.mp4" target="_blank" rel="noopener noreferrer"><img src="images/CSL.svg" height="12"></a><br>
						&#9679; 负责德国联邦经济事务和能源部 <a href="https://www.uni-bremen.de/csl/projekte/abgelaufene-projekte/arthrokinemat" target="_blank" rel="noopener noreferrer"><i>Arthrokinemat</i></a> 项目 <img src="images/Arthrokinemat.png" height="12" class="pop25"><br>
						&#9679; 研发首款实时识别人体动作、辅助康复的智能膝盖绷带<br>
						&#9679; <a href="https://biostec.scitevents.org/PreviousAwards.aspx#2019" target="_blank" rel="noopener noreferrer"><img src="images/BIOSTEC2019.png" height="12"> </a><a href="https://biostec.scitevents.org/PreviousAwards.aspx#2019" target="_blank" rel="noopener noreferrer">最佳论文奖</a>（学生作者）<img src="images/BestPaperAward.png" height="12" class="pop40"><br>
						&#9679; 建立广泛和密切的合作关系：<br>
						&nbsp;&nbsp; <a href="https://www.sport.kit.edu/Institut.php" target="_blank" rel="noopener noreferrer"><img src="images/KIT.png" height="12" class="pop5"></a> 卡尔斯鲁厄理工学院<br>
						&nbsp;&nbsp; <a href="https://www.bauerfeind.de/" target="_blank" rel="noopener noreferrer"><img src="images/Bauerfeind.png" height="12"  class="pop5"></a> <a href="https://www.bauerfeind.com.cn/" target="_blank" rel="noopener noreferrer">保尔范</a>运动护具公司等<br>
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.uni-bremen.de/" target="_blank" rel="noopener noreferrer"><img src="images/UB.png" height="40"></a>&nbsp;&nbsp;
						<a href="https://www.uni-bremen.de/csl" target="_blank" rel="noopener noreferrer"><img src="images/CSL.svg" height="40"></a><br>
						2021年以来<br>
						不来梅大学
					</td>
					<td>
						&#9679; 博士后研究员<br><br>
						&nbsp;&nbsp; <a href="https://www.uni-bremen.de/en/csl/institute/team/staff/dr-hui-liu" target="_blank" rel="noopener noreferrer"><img src="images/EN.svg" height="12" class="pop2"></a>
						&nbsp;&nbsp; <a href="https://www.uni-bremen.de/csl/institut/team/mitarbeiter/dr-hui-liu" target="_blank" rel="noopener noreferrer"><img src="images/DE.svg" height="12" class="pop2"></a>
						&nbsp;&nbsp;&nbsp; <a href="https://www.uni-bremen.de/csl/institut/team/mitarbeiter" target="_blank" rel="noopener noreferrer"><img src="images/CSLTeam.png" height="14" class="pop40"></a>
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.uni-bremen.de/en/" target="_blank" rel="noopener noreferrer"><img src="images/Nova.png" height="40"></a>&nbsp;&nbsp;
						<a href="https://www.uni-bremen.de/en/csl" target="_blank" rel="noopener noreferrer"><img src="images/LIBPhys-UNL.png" height="50"></a><br>
						2022年以来<br>
						新里斯本大学 LIBPhys<br>
						仪器、生医工和放射物理所
					</td>
					<td>
						&#9679; YERUN科研流动奖金资助的访问学者
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://cn.xauat.edu.cn/" target="_blank" rel="noopener noreferrer"><img src="images/XAUST.png" height="60"></a>&nbsp;&nbsp;
						<a href="https://ysxy.xauat.edu.cn/" target="_blank" rel="noopener noreferrer"><img src="images/XAUST_CoA.png" height="40"></a><br>
						2024年以来<br>
						西安建筑科学大学<br>
						艺术学院
					</td>
					<td>
						&#9679; 客座讲席教授
					</td>
				</tr>
			</table></td></tr>

			<tr><td></td></tr><tr><td></td></tr>

			<tr><td><h1 align="left"><span style="text-decoration: overline">教学工作</span></h1></td></tr>
			<tr><td><table align="left" cellpadding="15">
				<tr>
					<td align="center">
						<a href="https://www.uni-bremen.de/" target="_blank" rel="noopener noreferrer"><img src="images/UB.png" height="40"></a>&nbsp;&nbsp;
						<a href="https://www.uni-bremen.de/csl" target="_blank" rel="noopener noreferrer"><img src="images/CSL.svg" height="40"></a><br>
						2022年以来<br>
						<a href="https://www.uni-bremen.de/en/csl/teaching/winter-semester-2022-23/hot-topics-in-sensors-and-human-activity-research-03-ims-tshar" target="_blank" rel="noopener noreferrer"><b>传感器和人体动作研究</b></a>（英/德）
					</td>
					<td>
						&#9679; 课程负责人<br>
						&#9679; 研讨会和学习指导
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.uni-bremen.de/" target="_blank" rel="noopener noreferrer"><img src="images/UB.png" height="40"></a>&nbsp;&nbsp;
						<a href="https://www.uni-bremen.de/csl" target="_blank" rel="noopener noreferrer"><img src="images/CSL.svg" height="40"></a><br>
						2022年以来<br>
						<a href="https://www.uni-bremen.de/csl/lehre/wintersemester-2022-23/ausgewaehlte-probleme-kognitiver-systeme" target="_blank" rel="noopener noreferrer"><b>认知系统</b></a>（德）
					</td>
					<td>
						&#9679; 合作教学<br>
						&#9679; 研讨会和学习指导
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.uni-bremen.de/" target="_blank" rel="noopener noreferrer"><img src="images/UB.png" height="40"></a>&nbsp;&nbsp;
						<a href="https://www.uni-bremen.de/csl" target="_blank" rel="noopener noreferrer"><img src="images/CSL.svg" height="40"></a><br>
						2017年以来<br>
						<a href="https://www.uni-bremen.de/csl/lehre/sommersemester-2021/biosignale-und-benutzerschnittstellen" target="_blank" rel="noopener noreferrer"><b>生物医学信号和用户界面</b></a>（德）
					</td>
					<td>
						&#9679; 课程组织和电子教学平台、部分授课任务<br>
						&#9679; 习题课、线下和线上答疑<br>
						&#9679; 试卷命题人、口试委员<br>
						&#9679; 虚拟教学和线上同步指导负责人（疫情期间）
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.uni-bremen.de/" target="_blank" rel="noopener noreferrer"><img src="images/UB.png" height="40"></a>&nbsp;&nbsp;
						<a href="https://www.uni-bremen.de/csl" target="_blank" rel="noopener noreferrer"><img src="images/CSL.svg" height="40"></a><br>
						2020 — 2021<br>
						<a href="https://www.uni-bremen.de/csl/lehre/sommersemester-2021/grundlagen-des-maschinellen-lernens" target="_blank" rel="noopener noreferrer"><b>机器学习导论</b></a>（英）
					</td>
					<td>
						&#9679; 合作教学<br>
						&#9679; 网络教学视频录制和制作（疫情期间）<br>
						&#9679; 负责聚类分析和聚类算法
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.uni-bremen.de/" target="_blank" rel="noopener noreferrer"><img src="images/UB.png" height="40"></a>&nbsp;&nbsp;
						<a href="https://www.uni-bremen.de/csl" target="_blank" rel="noopener noreferrer"><img src="images/CSL.svg" height="40"></a><br>
						2020 — 2021<br>
						<a href="https://www.informatik.uni-bremen.de/projekttag/2021/projekte/robarinth/" target="_blank" rel="noopener noreferrer"><b>科学工程实践</b></a>: <a href="https://www.szi.uni-bremen.de/wp-content/uploads/2020/01/roboarinth.pdf" target="_blank" rel="noopener noreferrer"><i>RobARinth</i></a>（德）
					</td>
					<td>
						&#9679; 引导项目实践<br>
						&#9679; 提供生物信号传感器技术指导
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.uni-bremen.de/" target="_blank" rel="noopener noreferrer"><img src="images/UB.png" height="40"></a>&nbsp;&nbsp;
						<a href="https://www.uni-bremen.de/csl" target="_blank" rel="noopener noreferrer"><img src="images/CSL.svg" height="40"></a><br>
						2016 — 2018<br>
						<a href="http://www.informatik.uni-bremen.de/st/Lehre/swpII_1718/index.html" target="_blank" rel="noopener noreferrer"><b>软件工程II</b></a>（德）
					</td>
					<td>
						&#9679; 软件设计和软件工程教学<br>
						&#9679; 指导小组实践<br>
						&#9679; 组织参与客户会议和客户需求分析<br>
						&#9679; 指导程序设计和代码优化
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.sjtu.edu.cn/" target="_blank" rel="noopener noreferrer"><img src="images/SJTU.svg" width="60"></a><br>
						2010 — 2011<br>
						<a href="https://www.icourses.cn/sCourse/course_2929.html" target="_blank" rel="noopener noreferrer"><b>基本电路理论</b></a>（英）
					</td>
					<td>&#9679; 助教</td>
				</tr>
			</table></td></tr>
			
			<tr><td><table align="left" cellpadding="15">
				<tr><td><b>指导完成的有代表性的优秀本科、硕士毕业论文</b></td></tr>
				<tr><td><table align="left" cellpadding="15">
					<tr><td>2019</td><td>Yale Hartmann</td><td>Implementation and Optimisation of a Human Activity Recognition System using Sensors integrated into a Knee Bandage</td></tr>
					<tr><td>2019</td><td>Timo Urban</td><td>Entwicklung einer mobilen Anwendung für Echtzeit-Visualisierung und Archivierung von mehrkanaliger Biosignalaufnahme mit Bluetooth</td></tr>
					<tr><td>2019</td><td>Lennard Mai</td><td>Automatische Segmentierung von Bewegungsdaten eines biosignalbasierten, in einer Kniebandage integrierten HAR Systems</td></tr>
					<tr><td>2020</td><td>Yale Hartmann</td><td>Feature Selection for Multimodal Human Activity Recognition</td></tr>
					<tr><td>2021</td><td>Kilian Lüdemann</td><td>Sturzerkennung in Infrarotvideos mit Hilfe von Rekurrenten Neuronalen Netzen</td></tr>
					<tr><td>2021</td><td>Steffen Lahrberg</td><td>Direction Distinguishment in Wearable Sensor-Based Human Activity Recognition Using Cross-Channel Features</td></tr>
					<tr><td>2022</td><td>Tobias Dellert</td><td>Data Pipeline in AWS for Real-Time Workload-Monitoring Using HRV- and Inertial-Sensor Data via Mobile APP</td></tr>
					<tr><td>2022</td><td>Wei Wang</td><td>Towards Automatic Heart Sound Segmentation</td></tr>
					<tr><td>2023</td><td>Ciwan Gülpinar</td><td>Development of a Virtual Reality Application for the Real-Time Visualization and Archiving of Biosignals</td></tr>
					<tr><td>2024</td><td>Daniel Ott</td><td>Plattformübergreifende Software-Implementierung für individuelle Tonart-Anpassung mit Validierung der Brauchbarkeit</td></tr>
					<tr><td>2024</td><td>Leon Flaack</td><td>Klassifizierung von MIDI-Melodiespuren anhand von Long Short-term Memory-Netzwerken</td></tr>
					<tr><td>2024</td><td>Thorben Lorenzen</td><td>Erkennung von Finger-Aktivitäten beim Klavierspiel mit EMG</td></tr>
					<tr><td>2024</td><td>Hannes Wehrmann</td><td>Adaption von Musik an die Bewegungsdaten beim Laufen – Implementation und Validierung von ML-Algorithmen für Android-Applikationen</td></tr>
					<tr><td>2024</td><td>Shiyao Zhang</td><td>Real-Time Artifact Recognition during ECG Acquisition</td></tr>
					<tr><td>2024</td><td>Finn Redel</td><td>Entwicklung eines ereignisgesteuerten, verteilten Analyseframeworks für Chromatographiegerätedaten</td></tr>
					<tr><td>2024</td><td>Arthur Belousov</td><td>Melody-Based Songs’ Singing　Difficulty Analysis: Feature and Scoring Design, Web Platform Implementation, and Field Usability Study</td></tr>
				</table></td></tr>
			</table></td></tr>

			<tr><td><table align="left" cellpadding="15">
				<tr><td><b>指导的科研小组部分成员合影（2023/2024）</b></td></tr>
				<tr><td>
					<a href="images/students/Students.jpg" target="_blank" rel="noopener noreferrer"><img src="images/students/Students.jpg" height="400" class="slightpop"></a><br><br>
					左：Hasanur、Romina、Johann、Izzie、Daniel、Hannes、Abhinav、Rakibul<br>
					右：Nazia、Alenna、Md. Asif、Arthur、Rakubur、Robin、Shiyao、Jie、Ferdinand、Yvo
				</td></tr>
			</table></td></tr>

			<tr><td><table align="left" cellpadding="15">
				<tr>
					<td><b>指导的优秀科研助理以及毕设学生</b></a></td>
					<td align="center" colspan="2">
						<img src="images/DE.svg" height="40" class="round"> 
						<img src="images/BD.png" height="40" class="round"> 
						<img src="images/GE.png" height="40" class="round"> 
						<img src="images/PT.png" height="40" class="round"> 
						<img src="images/CN.png" height="40" class="round"> 
						<img src="images/IR.png" height="40" class="round"> 
						<img src="images/IN.png" height="40" class="round"> 
						<img src="images/KD.png" height="40" class="round"> 
						<img src="images/JP.png" height="40" class="round">
						<img src="images/VN.png" height="40" class="round">
						<img src="images/RU.png" height="40" class="round">
						<img src="images/TN.png" height="40" class="round">
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Yale_Hartmann.jpg" width="150" class="round"></a><br>
						&copy;<b>Yale Hartmann（德）</b>
					</td>
					<td>
						科研助理 2018 — 2020<br>
						本科毕设 2019 @ 德国不来梅（满分） <br>
						硕士毕设 2020 @ 德国不来梅（满分）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 人体动作识别；特征空间降维
						<br><br>					
						&rarr; 博士研究生 @ 不来梅大学
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Anindya_Chowdhury.jpg" width="150" class="round"></a><br>
						&copy;<b>Anindya Chowdhury（孟）</b>
					</td>
					<td>
						科研助理 2019<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 数据统计模型<br>
						<br><br>
						&rarr; 数据分析师 @ 富士胶片欧洲（拉廷根）
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Lennard_Mai.jpg" width="150" class="round"></a><br>
						&copy;<b>Lennard Mai（德）</b>
					</td>
					<td>
						本科毕设 2019 @ 德国不来梅<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 运动数据自动分割<br>
						<br><br>
						&rarr; 数据分析师 @ hmmh（不来梅）
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Timo_Urban.jpg" width="150" class="round"></a><br>
						&copy;<b>Timo Urban（德）</b>
					</td>
					<td>
						本科毕设 2019 @ 德国不来梅（满分）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 多模态生物信号采集的安卓应用开发
						<br><br>
						&rarr; 软件工程师 @ TK保险（汉堡）
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Male.png" width="150" class="round"></a><br>
						&copy;<b>Kilian Lüdemann（德）</b>
					</td>
					<td>
						本科毕设 2021 @ 德国不来梅（等级：“好”）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 基于RNN的红外视频摔倒识别
						<br><br>
						&rarr;  软件工程师 @ 埃森
						</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Tobias_Dellert.jpg" width="150" class="round"></a><br>
						&copy;<b>Tobias Dellert（德）</b>
					</td>
					<td>
						本科毕设 2022 @ 格鲁吉亚第比利斯<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 基于HRV和惯性传感的实时工作负载监控
						<br><br>
						&rarr; 数据工程师 @ WearHealth（不来梅）
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Isabel_de_Almeida_Curioso.jpg" width="150" class="round"></a><br>
						<b>Isabel de Almeida Curioso（葡）<br>&copy;NOVA FCT (Twitter)</b>
					</td>
					<td>
						硕士毕设 2022 @ 葡萄牙里斯本（满分（20），极难获得）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 临床环境下可靠的人工智能：丢失数据恢复
						<br><br>
						&rarr; 研究员 @ 葡萄牙
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Shiyao_Zhang.jpg" width="150" class="round"></a><br>
						&copy;<b>张诗瑶</b>
					</td>
					<td>
						科研助理和研究小组召集人 自2022<br>
						本科毕设 2024 @ 德国不来梅（满分） <br>
						硕士毕设 2025 @ 德国不来梅（满分）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 生物信号采集过程中的伪影识别<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 实时心电信号处理<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 实时肌电信号处理
						<br><br>
						&rarr; 博士研究生 @ 不来梅大学
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Steffen_Lahrberg.jpg" width="150" class="round"></a><br>
						&copy;<b>Steffen Lahrberg（德）</b>
					</td>
					<td>
						本科毕设 2022 @ 德国不来梅（满分）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 针对运动方向区分的跨信道特征设计
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Wei_Wang.jpg" width="150" class="round"></a><br>
						&copy;<b>王薇</b>
					</td>
					<td>
						硕士毕设 2022 @ 德国汉诺威<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 基于子序列搜索的自动心音信号分割
						<br><br>
						&rarr; IT应用开发师 @ 博世（爱森纳赫）
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Daniel_Osorio.jpg" width="150" class="round"></a><br>
						&copy;<b>Daniel Faustino de Noronha Osório（葡）</b>
					</td>
					<td>
						博士论文 2023 @ 葡萄牙里斯本<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 心电图普适遥感；心血管疾病评估
						<br><br>
						&rarr; 资深工程师 @ PLUX无线生物信号传感器公司
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Jonas_Boule.jpg" width="150" class="round"></a><br>
						&copy;<b>Jonas Boule（德）</b>
					</td>
					<td>
						科研助理 自2023<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 脑电信号分析<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 针对人体动作识别的脑电和惯性数据融合
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Romina_Razeghi_Oskouei.jpg" width="150" class="round"></a><br>
						&copy;<b>Romina Razeghi Oskouei（伊）</b>
					</td>
					<td>
						科研助理 自2023<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 肌电信号采集和处理<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 大数据<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 面部表情单元识别
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Ferdinand_Rohlfing.jpg" width="150" class="round"></a><br>
						@<b>Ferdinand Rohlfing（德）</b>
					</td>
					<td>
						科研助理 自2023<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 基于肌电的表情识别人工智能
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Anantha_Sripada_Abhinav_Veldanda.jpg" width="150" class="round"></a><br>
						<b>Anantha Sripada Abhinav Veldanda（印）</b>
					</td>
					<td>
						访问学生研究员<br>
						本科毕设 2023 @ 印度果阿 / 德国不来梅（A等）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 基于面部肌电信号的实时表情识别
						<br><br>
						&rarr; 工程师 @ 印度
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Ciwan_Gülpinar.jpg" width="150" class="round"></a><br>
						&copy;<b>Ciwan Gülpinar（德；库尔德裔）</b>
					</td>
					<td>
						本科毕设 2023 @ 德国不来梅（等级：“好”）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 元宇宙中生物信号的可视化和交互<br>
						<br><br>
						&rarr; 硕士研究生 @ 不来梅大学
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Johann_Joachim_Keuneke.jpg" width="150" class="round"></a><br>
						<b>Johann Joachim Keuneke（德）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 智能手机中的多模态传感器融合平台
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Daniel_Ott.jpg" width="150" class="round"></a><br>
						<b>Daniel Ott（德）</b>
					</td>
					<td>
						本科毕设 2023 @ 德国不来梅（等级：“非常优异”）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 演唱辅助的界面设计和用户体验分析
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Hasanur_Jaman_Seam.jpg" width="150" class="round"></a><br>
						&copy;<b>Hasanur Jaman Seam（孟）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 智能手机作为实时人体动作识别平台
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Hannes_Wehrmann.jpg" width="150" class="round"></a><br>
						<b>Hannes Wehrmann（德）</b>
					</td>
					<td>
						本科毕设 2024 @ 德国不来梅（等级：“好”）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 人体动作识别的可听化
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Rakibul_Islam.jpg" width="150" class="round"></a><br>
						&copy;<b>Rakibul Islam（孟）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 用于多信道联合信息检索的交互界面
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Thorben_Lorenzen.jpg" width="150" class="round"></a><br>
						&copy;<b>Thorben Lorenzen（德）</b>
					</td>
					<td>
						硕士毕设 2024 @ 德国不来梅（等级：“好”）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 针对钢琴自学的基于肌电的手指识别
						<br><br>
						&rarr; 数据工程师 @ 慕尼黑
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Leon_Flaack.jpg" width="150" class="round"></a><br>
						&copy;<b>Leon Flaack（德；日裔）</b>
					</td>
					<td>
						本科毕设 2024 @ 德国不来梅（等级：“好”）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 基于LSTM的MIDI主旋律通道自动识别
						<br><br>
						&rarr; 海外交换 @ 东京
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Jiumeng_Chen.jpg" width="150" class="round"></a><br>
						&copy;<b>陈久盟</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 足部动作识别
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Joel_Warnken.jpg" width="150" class="round"></a><br>
						&copy;<b>Joel Warnken（德）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 用于音准辅助训练的自动人声音高检测
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Mohamad_Tofayel_Ahmed.jpg" width="150" class="round"></a><br>
						&copy;<b>Mohamad Tofayel Ahmed（孟）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 燃料电池老化模型的人工智能修正
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Mohamad_Asif_Siddiqui.jpg" width="150" class="round"></a><br>
						&copy;<b>Mohamad Asif Siddiqui（孟）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 离群点检测算法在海量不同各类数据集上的性能评判 
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Arthur_Belousov.jpg" width="150" class="round"></a><br>
						<b>@Arthur Belousov（德）</b>
					</td>
					<td>
						科研助理 自2024<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 不来梅大数据竞赛秘书处<br><br>
						本科毕设 2023 @ 德国不来梅（等级：“非常优异”）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 歌曲演唱难度分析和用户界面
						<br><br>
						&rarr; 硕士研究生 @ 不来梅大学
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Aklima_Akter.jpg" width="150" class="round"></a><br>
						&copy;<b>Aklima Akter（孟）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 基于Wi-Fi人体动作识别中的高级数据处理
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Yvo_Muskulus.jpg" width="150" class="round"></a><br>
						&copy;<b>Yvo Muskulus（德）</b>
					</td>
					<td>
						科研助理 自2023<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 基于肌电的表情识别人工智能
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Alenna_Kha.jpg" width="150" class="round"></a><br>
						&copy;<b>Alenna Kha（德；越南裔）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 基于旋律的和弦自动生成
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Robin_Kuri.jpg" width="150" class="round"></a><br>
						&copy;<b>Robin Kuri（孟）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 从莫扎特骰子到“万能骰”
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/David_Czesla.jpg" width="150" class="round"></a><br>
						&copy;<b>David Czesla（德）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 基于手指和音高识别的钢琴自学反馈系统
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Jie_Su.jpg" width="150" class="round"></a><br>
						&copy;<b>苏杰</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; VR中的人体动作识别
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Nazia_Nusrat_Ima.jpg" width="150" class="round"></a><br>
						&copy;<b>Nazia Nusrat Ima（孟）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 孟加拉国独居老人社群拓展和线上活动APP
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Finn_Redel.jpg" width="150" class="round"></a><br>
						&copy;<b>Finn Redel（德）</b>
					</td>
					<td>
						本科毕设 2024 @ 德国不来梅<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 事件驱动的分布式色谱数据分析框架
						<br><br>
						&rarr; 工程师 @ 德国
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Rui_Varandas.jpg" width="150" class="round"></a><br>
						&copy;<b>Rui Pedro Sousa Varandas（葡）</b>
					</td>
					<td>
						博士论文 2023 @ 葡萄牙里斯本<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 人机交互中的人类学习过程评估
						<br><br>
						&rarr; 资深工程师 @ 里斯本
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Marcia_Ines_Almeida_Monteiro.jpg" width="150" class="round"></a><br>
						&copy;<b>Márcia Inês Almeida Monteiro（葡）</b>
					</td>
					<td>
						硕士毕设 2024 @ 葡萄牙里斯本（19/20，非常优异）<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; ECG噪声种类识别和质量评估
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/Ines_Nouili_Snoussi.jpg" width="150" class="round"></a><br>
						&copy;<b>Inès Nouili Snoussi（突）</b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 胃电在医疗中的应用
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/陶元兆.jpg" width="150" class="round"></a><br>
						&copy;<b>陶元兆</b>
					</td>
					<td>
						本科毕设 2025 @ 中国南京<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 股价预测
						<br><br>
						&rarr; 工业界 @ 安徽天长
					</td>
				</tr>

				<tr>
					<td align="center">
						<img src="images/students/何博宇.jpg" width="150" class="round"></a><br>
						&copy;<b>何博宇</b>
					</td>
					<td>
						本科毕设 2025 @ 中国南京<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 网站漏洞检测
						<br><br>
						&rarr; 工业界 @ 安徽合肥
					</td>
				</tr>
				
				<!--
				<tr>
					<td align="center">
						<img src="images/students/.jpg" width="150" class="round"></a><br>
						&copy;<b></b>
					</td>
					<td>
						毕设进行<br>
						&nbsp;&nbsp;&nbsp;&nbsp;&#9679; 
					</td>
				</tr>
				-->		

			</table></td></tr>

			
			<tr><td></td></tr><tr><td></td></tr>
		
			<tr><td><h3 align="left"><font face="sans" size="5">
				&nbsp;&nbsp;<a href="indexcn.html">首 页</a>&nbsp;&nbsp;&nbsp;&nbsp;
				<a>学 术</a>&nbsp;&nbsp;&nbsp;&nbsp;
				<a href="artcn.html">艺 术</a>&nbsp;&nbsp;&nbsp;&nbsp;
				<a href="academictreecn.html">学术族谱</a>&nbsp;&nbsp;&nbsp;&nbsp;
				<a href="arttreecn.html">艺术族谱</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
				<a href="research.html"><img src="images/EN.svg" height="15"></a>
			</font></h3></td></tr>

			<tr><td></td></tr>
		</table><br>
	</body>
</html>